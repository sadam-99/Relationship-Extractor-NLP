{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:10.824831Z",
     "start_time": "2020-11-26T22:22:03.705073Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:10.840462Z",
     "start_time": "2020-11-26T22:22:10.829402Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'                        # Directory for Data and Other files\n",
    "ckpt_dir = '../checkpoint'                  # Directory for Checkpoints \n",
    "word_embd_dir = '../checkpoint/word_embd'   # Directory for Checkpoints of Word Embedding Layer\n",
    "model_dir = '../checkpoint/modelv1'         # Directory for Checkpoints of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:10.870578Z",
     "start_time": "2020-11-26T22:22:10.843946Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embd_dim = 100       # Dimension of embedding layer for words\n",
    "pos_embd_dim = 25         # Dimension of embedding layer for POS Tags\n",
    "dep_embd_dim = 25         # Dimension of embedding layer for Dependency Types\n",
    "\n",
    "word_vocab_size = 400001  # Vocab size for Words\n",
    "pos_vocab_size = 10       # Vocab size for POS Tags\n",
    "dep_vocab_size = 21       # Vocab size for Dependency Types\n",
    "word_state_size = 100\n",
    "other_state_size = 100\n",
    "relation_classes = 19     # No. of Relation Classes\n",
    "state_size = 100          # Dimension of States of LSTM-RNNs\n",
    "batch_size = 10           # Batch Size for training\n",
    "\n",
    "channels = 100      # No. of types of features to feed in LSTM-RNN\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 10         # Maximum length of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:10.917085Z",
     "start_time": "2020-11-26T22:22:10.875691Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "grap = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:37.546708Z",
     "start_time": "2020-11-26T22:22:37.527746Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    with tf.name_scope(\"input\"):\n",
    "    \n",
    "    # Length of the sequence = 2X10\n",
    "        path_length = tf.placeholder(tf.int32, shape=[2, batch_size], name=\"path1_length\") \n",
    "\n",
    "        # Words in the sequence  = 2X10X10\n",
    "        word_ids = tf.placeholder(tf.int32, shape=[2, batch_size, max_len_path], name=\"word_ids\") \n",
    "\n",
    "         # POS Tags in the sequence = 2X10X10\n",
    "        pos_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"pos_ids\") \n",
    "\n",
    "        # Dependency Types in the sequence = 2X10X10\n",
    "        dep_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"dep_ids\") \n",
    "\n",
    "         # True Relation btw the entities = [10]\n",
    "        y = tf.placeholder(tf.int32, [batch_size], name=\"y\")   \n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:39.653527Z",
     "start_time": "2020-11-26T22:22:39.624952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'input/word_ids:0' shape=(2, 10, 10) dtype=int32>,\n",
       " <tf.Tensor 'input/dep_ids:0' shape=(2, 10, 10) dtype=int32>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids, dep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:54.479490Z",
     "start_time": "2020-11-26T22:22:54.364916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Layer of Words \n",
    "with grap.as_default():\n",
    "    with tf.name_scope(\"word_embedding\"):\n",
    "        W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "        embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "        embedding_init = W.assign(embedding_placeholder)\n",
    "        embedded_word = tf.nn.embedding_lookup(W, word_ids)\n",
    "        word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    # Embedding Layer of POS Tags \n",
    "    with tf.name_scope(\"pos_embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "        embedded_pos = tf.nn.embedding_lookup(W, pos_ids)\n",
    "        pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    # Embedding Layer of Dependency Types \n",
    "    with tf.name_scope(\"dep_embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "        embedded_dep = tf.nn.embedding_lookup(W, dep_ids)\n",
    "        dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:37:39.764035Z",
     "start_time": "2020-11-27T01:37:39.754173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'pos_embedding/W:0' shape=(10, 25) dtype=float32_ref>, <tf.Variable 'pos_embedding/W/Adam:0' shape=(10, 25) dtype=float32_ref>, <tf.Variable 'pos_embedding/W/Adam_1:0' shape=(10, 25) dtype=float32_ref>, <tf.Variable 'pos_embedding/W/Adam_2:0' shape=(10, 25) dtype=float32_ref>, <tf.Variable 'pos_embedding/W/Adam_3:0' shape=(10, 25) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    my_scope = 'pos_embedding'\n",
    "    scope_variables=  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=my_scope )\n",
    "    print(scope_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:56.399315Z",
     "start_time": "2020-11-26T22:22:56.378098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.saver.Saver at 0x1d363ef6d48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embedding_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:22:57.912861Z",
     "start_time": "2020-11-26T22:22:57.883049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'strided_slice:0' shape=(10, 10, 100) dtype=float32>,\n",
       " <tf.Tensor 'pos_embedding/embedding_lookup/Identity:0' shape=(2, 10, 10, 25) dtype=float32>,\n",
       " <tf.Tensor 'dep_embedding/embedding_lookup/Identity:0' shape=(2, 10, 10, 25) dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_word[0], embedded_pos, embedded_dep\n",
    "# type(embedded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T04:01:29.774525Z",
     "start_time": "2020-11-23T04:01:29.763823Z"
    }
   },
   "outputs": [],
   "source": [
    "# type(embedded_word)\n",
    "# with tf.Session() as ss:\n",
    "#     print(embedded_word.eval())\n",
    "# tf.Print(embedded_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T04:01:32.475059Z",
     "start_time": "2020-11-23T04:01:32.460148Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:23:27.510297Z",
     "start_time": "2020-11-20T07:23:27.504242Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:22:25.876899Z",
     "start_time": "2020-11-20T07:22:25.292371Z"
    }
   },
   "outputs": [],
   "source": [
    "# sz_sequence = 100\n",
    "# n_units = 120\n",
    "# n_layers = 3\n",
    "# with tf.variable_scope(\"word_lstm1\"):\n",
    "#     inputs = tf.keras.layers.Input(batch_shape=(batch_size, sz_sequence, channels))\n",
    "#     cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "#     state_series = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)\n",
    "    \n",
    "    \n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=init_states[0])\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:49:20.056735Z",
     "start_time": "2020-11-20T07:49:19.976990Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a BasicRNNCell\n",
    "# rnn_cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "# # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "\n",
    "# # defining initial state\n",
    "# initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "# outputs, state = tf.compat.v1.nn.dynamic_rnn(rnn_cell, embedded_word[0],\n",
    "#                                    initial_state=initial_state,\n",
    "#                                    dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LSTM Layers for Different Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T08:10:43.314330Z",
     "start_time": "2020-11-20T08:10:42.953429Z"
    }
   },
   "outputs": [],
   "source": [
    "# sz_sequence = 10\n",
    "# n_units = 100\n",
    "# n_layers = 3\n",
    "# tf.reset_default_graph()\n",
    "# #  For Word Embeddings\n",
    "# with tf.variable_scope(\"word_lstm1\"):\n",
    "#     inputs = tf.keras.layers.Input(batch_shape=(batch_size, sz_sequence, channels))\n",
    "#     cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "#     state_series = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T03:43:41.570008Z",
     "start_time": "2020-11-23T03:43:41.565773Z"
    }
   },
   "outputs": [],
   "source": [
    "# state_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:23:21.118258Z",
     "start_time": "2020-11-26T22:23:19.043378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "state_series_w1 Tensor(\"word_lstm1/rnn/transpose_1:0\", shape=(10, 10, 100), dtype=float32)\n",
      "state_series_word1 Tensor(\"word_lstm1/Max:0\", shape=(10, 100), dtype=float32)\n",
      "state_series_w2 Tensor(\"word_lstm2/rnn_1/transpose_1:0\", shape=(10, 10, 100), dtype=float32)\n",
      "state_series_word2 Tensor(\"word_lstm2/Max:0\", shape=(10, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word_sz_sequence = 10\n",
    "n_units = 100\n",
    "n_layers = 3\n",
    "channels_word=100\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# with grap.as_default():\n",
    "#  For Word Embeddings\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"word_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, word_sz_sequence, channels_word))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_w1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_word[0])\n",
    "        state_series_word1 = tf.reduce_max(state_series_w1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_w1\", state_series_w1)\n",
    "        print(\"state_series_word1\", state_series_word1)\n",
    "    #  For Word Embeddings 2\n",
    "    with tf.variable_scope(\"word_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, word_sz_sequence, channels_word))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_w2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_word[1])\n",
    "        state_series_word2 = tf.reduce_max(state_series_w1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_w2\", state_series_w2)\n",
    "        print(\"state_series_word2\", state_series_word2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T07:52:16.280116Z",
     "start_time": "2020-11-23T07:52:16.261105Z"
    }
   },
   "outputs": [],
   "source": [
    "# n_units=100\n",
    "# n_layers=3\n",
    "# cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "# cells[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:23:33.365780Z",
     "start_time": "2020-11-26T22:23:31.533080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series_p1 Tensor(\"pos_lstm1/rnn_2/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_pos1 Tensor(\"pos_lstm1/Max:0\", shape=(10, 25), dtype=float32)\n",
      "state_series_w2 Tensor(\"pos_lstm2/rnn_3/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_pos2 Tensor(\"pos_lstm2/Max:0\", shape=(10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  For POS Embeddings 1\n",
    "pos_sz_sequence_POS = 10\n",
    "channels_POS = 25\n",
    "n_units_POS = 25\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"pos_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence_POS, channels_POS)) # 10X10X25\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_POS) for _ in range(n_layers)]\n",
    "        state_series_p1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_pos[0])\n",
    "        state_series_pos1 = tf.reduce_max(state_series_p1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_p1\", state_series_p1)\n",
    "        print(\"state_series_pos1\", state_series_pos1)\n",
    "\n",
    "    #  For POS Embeddings 2\n",
    "    with tf.variable_scope(\"pos_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence_POS, channels_POS)) # 10X25X3\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_POS) for _ in range(n_layers)]\n",
    "        state_series_p2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_pos[1])\n",
    "        state_series_pos2 = tf.reduce_max(state_series_p2, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_w2\", state_series_p2)\n",
    "        print(\"state_series_pos2\", state_series_pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:23:48.209934Z",
     "start_time": "2020-11-26T22:23:48.194077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'pos_lstm2/rnn_3/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable_2:0' shape=(10, 25) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    my_scope = 'pos_lstm2'\n",
    "    scope_variables=  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=my_scope )\n",
    "    print(scope_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:24:05.830597Z",
     "start_time": "2020-11-26T22:24:04.112198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series_d1 Tensor(\"dep_lstm1/rnn_4/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_dep1 Tensor(\"dep_lstm1/Max:0\", shape=(10, 25), dtype=float32)\n",
      "state_series_d2 Tensor(\"dep_lstm2/rnn_5/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_dep2 Tensor(\"dep_lstm2/Max:0\", shape=(10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  For POS Embeddings 2\n",
    "dep_sz_sequence = 10\n",
    "channels_DEP = 25\n",
    "n_units_DEP = 25\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"dep_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, dep_sz_sequence, channels_DEP))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_DEP) for _ in range(n_layers)]\n",
    "        state_series_d1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_dep[0])\n",
    "        state_series_dep1 = tf.reduce_max(state_series_d1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_d1\", state_series_d1)\n",
    "        print(\"state_series_dep1\", state_series_dep1)\n",
    "\n",
    "    with tf.variable_scope(\"dep_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, dep_sz_sequence, channels_DEP))#10X10X25\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_DEP) for _ in range(n_layers)]\n",
    "        state_series_d2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_dep[1])\n",
    "        state_series_dep2 = tf.reduce_max(state_series_d2, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_d2\", state_series_d2)\n",
    "        print(\"state_series_dep2\", state_series_dep2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T04:00:29.241274Z",
     "start_time": "2020-11-23T04:00:29.230157Z"
    }
   },
   "outputs": [],
   "source": [
    "ce = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "ce[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:24:17.898213Z",
     "start_time": "2020-11-26T22:24:17.862373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series Tensor(\"concat_2:0\", shape=(10, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "    state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)\n",
    "\n",
    "    state_series = tf.concat([state_series1, state_series2], 1)\n",
    "    init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "    print(\"state_series\", state_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated RNN-LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T09:40:29.653977Z",
     "start_time": "2020-11-19T09:40:29.577049Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hidden_states = tf.zeros([channels, batch_size, state_size], name=\"hidden_state\")\n",
    "# cell_states = tf.zeros([channels, batch_size, state_size], name=\"cell_state\")\n",
    "\n",
    "# init_states = [tf.contrib.rnn.LSTMStateTuple(hidden_states[i], cell_states[i]) for i in range(channels)]\n",
    "# tf.reset_default_graph()\n",
    "# #  For Word Embeddings\n",
    "# with tf.variable_scope(\"word_lstm1\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=init_states[0])\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For Word Embeddings 2\n",
    "# with tf.variable_scope(\"word_lstm2\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[1], sequence_length=path_length[1], initial_state=init_states[0])\n",
    "#     state_series_word2 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For POS Embeddings 1\n",
    "# with tf.variable_scope(\"pos_lstm1\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[0], sequence_length=path_length[0],initial_state=init_states[1])\n",
    "#     state_series_pos1 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For POS Embeddings 2\n",
    "# with tf.variable_scope(\"pos_lstm2\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[1], sequence_length=path_length[1],initial_state=init_states[1])\n",
    "#     state_series_pos2 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For POS Embeddings 2\n",
    "# with tf.variable_scope(\"dep_lstm1\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[0], sequence_length=path_length[0], initial_state=init_states[2])\n",
    "#     state_series_dep1 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "# with tf.variable_scope(\"dep_lstm2\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[1], sequence_length=path_length[1], initial_state=init_states[2])\n",
    "#     state_series_dep2 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "# state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "# state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)\n",
    "\n",
    "# state_series = tf.concat([state_series1, state_series2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:19.567714Z",
     "start_time": "2020-11-21T09:24:19.541748Z"
    }
   },
   "outputs": [],
   "source": [
    "# embedded_dep[0], embedded_word\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:34:13.607423Z",
     "start_time": "2020-11-26T21:34:13.593674Z"
    }
   },
   "outputs": [],
   "source": [
    "state_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:18:07.340674Z",
     "start_time": "2020-11-27T01:18:07.283944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"softmax_layer_1/add:0\", shape=(10, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# grap = tf.Graph()\n",
    "with grap.as_default():\n",
    "    with tf.name_scope(\"hidden_layer\"):\n",
    "        W = tf.Variable(tf.truncated_normal([300, 100], -0.1, 0.1), name=\"W\")\n",
    "#         b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "        b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "        y_hidden_layer = tf.matmul(state_series, W) + b\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    with tf.name_scope(\"softmax_layer\"):\n",
    "        W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "        logits = tf.matmul(y_hidden_layer, W) + b\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:22:59.523226Z",
     "start_time": "2020-11-27T01:22:59.504605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer/W:0\n",
      "hidden_layer/b:0\n",
      "hidden_layer/W/Adam:0\n",
      "hidden_layer/W/Adam_1:0\n",
      "hidden_layer/b/Adam:0\n",
      "hidden_layer/b/Adam_1:0\n",
      "hidden_layer_1/W:0\n",
      "hidden_layer_2/W:0\n",
      "hidden_layer_2/b:0\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    my_scope = 'hidden_layer'\n",
    "    scope_variables=  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope= my_scope )\n",
    "    for scope_var in scope_variables:\n",
    "        print(scope_var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     # True Relation btw the entities\n",
    "#     y = tf.placeholder(tf.int32, [batch_size], name=\"y\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:21:14.118562Z",
     "start_time": "2020-11-27T01:21:14.114591Z"
    }
   },
   "outputs": [],
   "source": [
    "# tv_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:34:37.203191Z",
     "start_time": "2020-11-27T01:34:37.180813Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    tv_all = tf.trainable_variables()\n",
    "    tv_regu = []\n",
    "    non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "    for t in tv_all:\n",
    "        if t.name not in non_reg:\n",
    "#             if(t.name.find('biases')==-1):\n",
    "            if(t.name.find('bias')==-1):\n",
    "                tv_regu.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:34:40.610875Z",
     "start_time": "2020-11-27T01:34:40.594344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_lstm1/rnn/kernel:0\n",
      "word_lstm1/rnn/recurrent_kernel:0\n",
      "word_lstm1/rnn/kernel_1:0\n",
      "word_lstm1/rnn/recurrent_kernel_1:0\n",
      "word_lstm1/rnn/kernel_2:0\n",
      "word_lstm1/rnn/recurrent_kernel_2:0\n",
      "word_lstm1/rnn/Variable:0\n",
      "word_lstm1/rnn/Variable_1:0\n",
      "word_lstm1/rnn/Variable_2:0\n",
      "word_lstm2/rnn_1/kernel:0\n",
      "word_lstm2/rnn_1/recurrent_kernel:0\n",
      "word_lstm2/rnn_1/kernel_1:0\n",
      "word_lstm2/rnn_1/recurrent_kernel_1:0\n",
      "word_lstm2/rnn_1/kernel_2:0\n",
      "word_lstm2/rnn_1/recurrent_kernel_2:0\n",
      "word_lstm2/rnn_1/Variable:0\n",
      "word_lstm2/rnn_1/Variable_1:0\n",
      "word_lstm2/rnn_1/Variable_2:0\n",
      "pos_lstm1/rnn_2/kernel:0\n",
      "pos_lstm1/rnn_2/recurrent_kernel:0\n",
      "pos_lstm1/rnn_2/kernel_1:0\n",
      "pos_lstm1/rnn_2/recurrent_kernel_1:0\n",
      "pos_lstm1/rnn_2/kernel_2:0\n",
      "pos_lstm1/rnn_2/recurrent_kernel_2:0\n",
      "pos_lstm1/rnn_2/Variable:0\n",
      "pos_lstm1/rnn_2/Variable_1:0\n",
      "pos_lstm1/rnn_2/Variable_2:0\n",
      "pos_lstm2/rnn_3/kernel:0\n",
      "pos_lstm2/rnn_3/recurrent_kernel:0\n",
      "pos_lstm2/rnn_3/kernel_1:0\n",
      "pos_lstm2/rnn_3/recurrent_kernel_1:0\n",
      "pos_lstm2/rnn_3/kernel_2:0\n",
      "pos_lstm2/rnn_3/recurrent_kernel_2:0\n",
      "pos_lstm2/rnn_3/Variable:0\n",
      "pos_lstm2/rnn_3/Variable_1:0\n",
      "pos_lstm2/rnn_3/Variable_2:0\n",
      "dep_lstm1/rnn_4/kernel:0\n",
      "dep_lstm1/rnn_4/recurrent_kernel:0\n",
      "dep_lstm1/rnn_4/kernel_1:0\n",
      "dep_lstm1/rnn_4/recurrent_kernel_1:0\n",
      "dep_lstm1/rnn_4/kernel_2:0\n",
      "dep_lstm1/rnn_4/recurrent_kernel_2:0\n",
      "dep_lstm1/rnn_4/Variable:0\n",
      "dep_lstm1/rnn_4/Variable_1:0\n",
      "dep_lstm1/rnn_4/Variable_2:0\n",
      "dep_lstm2/rnn_5/kernel:0\n",
      "dep_lstm2/rnn_5/recurrent_kernel:0\n",
      "dep_lstm2/rnn_5/kernel_1:0\n",
      "dep_lstm2/rnn_5/recurrent_kernel_1:0\n",
      "dep_lstm2/rnn_5/kernel_2:0\n",
      "dep_lstm2/rnn_5/recurrent_kernel_2:0\n",
      "dep_lstm2/rnn_5/Variable:0\n",
      "dep_lstm2/rnn_5/Variable_1:0\n",
      "dep_lstm2/rnn_5/Variable_2:0\n",
      "hidden_layer/W:0\n",
      "softmax_layer/W:0\n",
      "hidden_layer_1/W:0\n",
      "hidden_layer_2/W:0\n",
      "hidden_layer_2/b:0\n",
      "softmax_layer_1/W:0\n",
      "softmax_layer_1/b:0\n"
     ]
    }
   ],
   "source": [
    "for tv in tv_regu:\n",
    "    print(tv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:01.468639Z",
     "start_time": "2020-11-27T01:34:53.772289Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        total_loss = loss + l2_loss\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)\n",
    "    init=tf.global_variables_initializer()  # Initializing the optimizer variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:04.238839Z",
     "start_time": "2020-11-27T01:35:04.224467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'word_embedding/W:0' shape=(400001, 100) dtype=float32_ref>, <tf.Variable 'pos_embedding/W:0' shape=(10, 25) dtype=float32_ref>, <tf.Variable 'dep_embedding/W:0' shape=(21, 25) dtype=float32_ref>, <tf.Variable 'word_lstm1/rnn/kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/recurrent_kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/bias:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm1/rnn/kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/recurrent_kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/bias_1:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm1/rnn/kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/recurrent_kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/bias_2:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm1/rnn/Variable:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm1/rnn/Variable_1:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm1/rnn/Variable_2:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/recurrent_kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/bias:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/recurrent_kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/bias_1:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/recurrent_kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/bias_2:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/Variable:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/Variable_1:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/Variable_2:0' shape=(10, 100) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'hidden_layer/W:0' shape=(300, 100) dtype=float32_ref>, <tf.Variable 'hidden_layer/b:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'softmax_layer/W:0' shape=(100, 19) dtype=float32_ref>, <tf.Variable 'softmax_layer/b:0' shape=(19,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>, <tf.Variable 'hidden_layer_1/W:0' shape=(300, 100) dtype=float32_ref>, <tf.Variable 'hidden_layer_2/W:0' shape=(300, 100) dtype=float32_ref>, <tf.Variable 'hidden_layer_2/b:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'softmax_layer_1/W:0' shape=(100, 19) dtype=float32_ref>, <tf.Variable 'softmax_layer_1/b:0' shape=(19,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "### y, logits\n",
    "# tv_regu\n",
    "print(tv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:10.566127Z",
     "start_time": "2020-11-27T01:35:10.250061Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(data_dir + '/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token\n",
    "\n",
    "pos_tags_vocab = []\n",
    "for line in open(data_dir + '/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())\n",
    "\n",
    "dep_vocab = []\n",
    "for line in open(data_dir + '/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "relation_vocab = []\n",
    "for line in open(data_dir + '/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())\n",
    "\n",
    "\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'\n",
    "\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:12.350952Z",
     "start_time": "2020-11-27T01:35:12.334637Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(data_dir + '/train_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/train_relations.txt'):\n",
    "#     print(line.strip().split())\n",
    "    relations.append(line.strip().split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:14.131147Z",
     "start_time": "2020-11-27T01:35:14.118734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Component-Whole(e2,e1)', 'Other', 'Instrument-Agency(e2,e1)', 'Other', 'Member-Collection(e1,e2)', 'Other', 'Cause-Effect(e2,e1)', 'Entity-Destination(e1,e2)', 'Content-Container(e1,e2)', 'Entity-Destination(e1,e2)', 'Member-Collection(e1,e2)', 'Other', 'Message-Topic(e1,e2)', 'Cause-Effect(e2,e1)', 'Instrument-Agency(e2,e1)', 'Message-Topic(e1,e2)', 'Instrument-Agency(e2,e1)', 'Product-Producer(e2,e1)', 'Component-Whole(e2,e1)', 'Member-Collection(e2,e1)', 'Entity-Origin(e1,e2)', 'Member-Collection(e2,e1)', 'Cause-Effect(e1,e2)', 'Other', 'Member-Collection(e2,e1)', 'Other', 'Cause-Effect(e1,e2)', 'Message-Topic(e1,e2)', 'Message-Topic(e1,e2)', 'Component-Whole(e1,e2)', 'Message-Topic(e2,e1)', 'Cause-Effect(e2,e1)', 'Product-Producer(e1,e2)', 'Entity-Destination(e1,e2)', 'Component-Whole(e1,e2)', 'Entity-Origin(e1,e2)', 'Other', 'Component-Whole(e2,e1)', 'Cause-Effect(e1,e2)', 'Instrument-Agency(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:35:13.431304Z",
     "start_time": "2020-11-26T21:35:13.415770Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_p1\n",
    "# max_len_path\n",
    "# word_p1_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:18.537729Z",
     "start_time": "2020-11-27T01:35:18.502259Z"
    }
   },
   "outputs": [],
   "source": [
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:23.396091Z",
     "start_time": "2020-11-27T01:35:23.383416Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(defer_build=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:55:15.113727Z",
     "start_time": "2020-11-26T21:55:15.099798Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dep_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:55:06.156470Z",
     "start_time": "2020-11-26T21:55:06.148278Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')\n",
    "for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dep_embedding'):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T08:30:30.844185Z",
     "start_time": "2020-11-23T08:30:30.244283Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_dir = r\"E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\checkpoint\\modelv1\"\n",
    "model = tf.train.latest_checkpoint(model_dir)\n",
    "saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:25:48.051411Z",
     "start_time": "2020-11-26T22:25:48.045184Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:35:28.804790Z",
     "start_time": "2020-11-26T21:35:28.784846Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "# word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:29:51.554040Z",
     "start_time": "2020-11-21T09:29:51.533865Z"
    }
   },
   "outputs": [],
   "source": [
    "# grap.version\n",
    "from keras import backend as K\n",
    "\n",
    "#Before prediction\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:35:36.659767Z",
     "start_time": "2020-11-26T21:35:33.664120Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.convert_to_tensor(word_dict)\n",
    "# word_dict\n",
    "# grap\n",
    "import numpy as np\n",
    "from tensorflow.python.layers import base\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:48:45.238487Z",
     "start_time": "2020-11-26T21:48:45.227688Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:46:59.238340Z",
     "start_time": "2020-11-26T21:46:59.228597Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T08:14:23.487551Z",
     "start_time": "2020-11-23T08:14:23.461730Z"
    }
   },
   "outputs": [],
   "source": [
    "# path_dict\n",
    "# type(y_dict)\n",
    "# word_dict[0].shape\n",
    "# dep_dict[0].shape\n",
    "# path_dict[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:35:43.204713Z",
     "start_time": "2020-11-26T21:35:43.193218Z"
    }
   },
   "outputs": [],
   "source": [
    "# relations\n",
    "sess = tf.Session()\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:38:04.053688Z",
     "start_time": "2020-11-26T21:38:04.042918Z"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:55.524879Z",
     "start_time": "2020-11-27T01:35:51.691691Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.global_variables_initializer()\n",
    "# init_op = tf.initialize_all_variables()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init_op)\n",
    "with grap.as_default():\n",
    "    init_g = tf.global_variables_initializer()\n",
    "    init_l = tf.local_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_g)\n",
    "        sess.run(init_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:05:40.116864Z",
     "start_time": "2020-11-26T22:05:40.095760Z"
    }
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:11:29.652237Z",
     "start_time": "2020-11-26T22:11:29.637490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add an op to initialize the variables.\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Later, when launching the model\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(init_op))\n",
    "  # Run the init operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:36:16.512989Z",
     "start_time": "2020-11-27T01:36:11.597992Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sess Running\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value pos_embedding/W/Adam_2\n\t [[node pos_embedding/W/Adam_2/read (defined at <ipython-input-47-133c61df682e>:10) ]]\n\nOriginal stack trace for 'pos_embedding/W/Adam_2/read':\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-47-133c61df682e>\", line 10, in <module>\n    optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 413, in minimize\n    name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 131, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 1155, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 190, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 164, in create_slot_with_initializer\n    dtype)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 74, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 562, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1755, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 4996, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value pos_embedding/W/Adam_2\n\t [[{{node pos_embedding/W/Adam_2/read}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-1fa30f350a4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrap\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sess Running\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value pos_embedding/W/Adam_2\n\t [[node pos_embedding/W/Adam_2/read (defined at <ipython-input-47-133c61df682e>:10) ]]\n\nOriginal stack trace for 'pos_embedding/W/Adam_2/read':\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-47-133c61df682e>\", line 10, in <module>\n    optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 413, in minimize\n    name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 597, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 131, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 1155, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 190, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 164, in create_slot_with_initializer\n    dtype)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 74, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1496, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1239, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 562, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 514, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 929, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 259, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 220, in _variable_v1_call\n    shape=shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 198, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2511, in default_variable_creator\n    shape=shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 263, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1568, in __init__\n    shape=shape)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1755, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 86, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 4996, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "        word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "#         path_dict = tf.convert_to_tensor(path_dict)\n",
    "#         word_dict = tf.convert_to_tensor(word_dict)\n",
    "#         pos_dict = tf.convert_to_tensor(pos_dict)\n",
    "#         dep_dict = tf.convert_to_tensor(dep_dict)\n",
    "#         y_dict = tf.convert_to_tensor(y_dict)\n",
    "        \n",
    "        feed_dict = {\n",
    "            path_length:path_dict,\n",
    "            word_ids:word_dict,\n",
    "            pos_ids:pos_dict,\n",
    "            dep_ids:dep_dict,\n",
    "            y:y_dict}\n",
    "        with grap.as_default():\n",
    "            with tf.Session(graph=grap) as sess:\n",
    "                print(\"Sess Running\")\n",
    "                _, loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "            if step%10==0:\n",
    "                print(\"Step:\", step, \"loss:\",loss)\n",
    "            if step % 1000 == 0:\n",
    "                saver.save(sess, model_dir + '/model')\n",
    "                print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"training accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_dir + '/test_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/test_relations.txt'):\n",
    "    relations.append(line.strip().split()[0])\n",
    "\n",
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T04:29:15.291700Z",
     "start_time": "2020-11-23T04:29:15.277598Z"
    }
   },
   "outputs": [],
   "source": [
    "y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"test accuracy\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
