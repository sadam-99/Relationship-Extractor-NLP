{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:17.590845Z",
     "start_time": "2020-11-28T05:15:09.726666Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:19.375636Z",
     "start_time": "2020-11-28T05:15:19.369086Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'                        # Directory for Data and Other files\n",
    "ckpt_dir = '../checkpoint'                  # Directory for Checkpoints \n",
    "word_embd_dir = '../checkpoint/word_embd'   # Directory for Checkpoints of Word Embedding Layer\n",
    "# model_dir = '../checkpoint/modelv1'         # Directory for Checkpoints of Model\n",
    "model_DIR = r\"checkpoint/modelv1\" # Directory for Checkpoints of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:21.874172Z",
     "start_time": "2020-11-28T05:15:21.851835Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embd_dim = 100       # Dimension of embedding layer for words\n",
    "pos_embd_dim = 25         # Dimension of embedding layer for POS Tags\n",
    "dep_embd_dim = 25         # Dimension of embedding layer for Dependency Types\n",
    "\n",
    "word_vocab_size = 400001  # Vocab size for Words\n",
    "pos_vocab_size = 10       # Vocab size for POS Tags\n",
    "dep_vocab_size = 21       # Vocab size for Dependency Types\n",
    "word_state_size = 100\n",
    "other_state_size = 100\n",
    "relation_classes = 19     # No. of Relation Classes\n",
    "state_size = 100          # Dimension of States of LSTM-RNNs\n",
    "batch_size = 10           # Batch Size for training\n",
    "\n",
    "channels = 100      # No. of types of features to feed in LSTM-RNN\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 10         # Maximum length of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:24.835606Z",
     "start_time": "2020-11-28T05:15:24.801386Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "grap = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:32.529111Z",
     "start_time": "2020-11-28T05:15:32.506909Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    with tf.name_scope(\"input\"):\n",
    "    \n",
    "    # Length of the sequence = 2X10\n",
    "        path_length = tf.placeholder(tf.int32, shape=[2, batch_size], name=\"path1_length\") \n",
    "\n",
    "        # Words in the sequence  = 2X10X10\n",
    "        word_ids = tf.placeholder(tf.int32, shape=[2, batch_size, max_len_path], name=\"word_ids\") \n",
    "\n",
    "         # POS Tags in the sequence = 2X10X10\n",
    "        pos_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"pos_ids\") \n",
    "\n",
    "        # Dependency Types in the sequence = 2X10X10\n",
    "        dep_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"dep_ids\") \n",
    "\n",
    "         # True Relation btw the entities = [10]\n",
    "        y = tf.placeholder(tf.int32, [batch_size], name=\"y\")   \n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:11:50.282847Z",
     "start_time": "2020-11-28T02:11:50.266125Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_ids, dep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:39.242172Z",
     "start_time": "2020-11-28T05:15:39.075438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Layer of Words \n",
    "with grap.as_default():\n",
    "    with tf.name_scope(\"word_embedding\"):\n",
    "        W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "        embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "        embedding_init = W.assign(embedding_placeholder)\n",
    "        embedded_word = tf.nn.embedding_lookup(W, word_ids)\n",
    "        word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    # Embedding Layer of POS Tags \n",
    "    with tf.name_scope(\"pos_embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "        embedded_pos = tf.nn.embedding_lookup(W, pos_ids)\n",
    "        pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    # Embedding Layer of Dependency Types \n",
    "    with tf.name_scope(\"dep_embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "        embedded_dep = tf.nn.embedding_lookup(W, dep_ids)\n",
    "        dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:11:55.742631Z",
     "start_time": "2020-11-28T02:11:55.723119Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    my_scope = 'pos_embedding'\n",
    "    scope_variables=  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=my_scope )\n",
    "    print(scope_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T01:52:23.350058Z",
     "start_time": "2020-11-28T01:52:23.324835Z"
    }
   },
   "outputs": [],
   "source": [
    "# pos_embedding_saver\n",
    "# embedded_word[0], embedded_pos, embedded_dep\n",
    "# type(embedded_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LSTM Layers for Different Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:47.017782Z",
     "start_time": "2020-11-28T05:15:45.145420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "state_series_w1 Tensor(\"word_lstm1/rnn/transpose_1:0\", shape=(10, 10, 100), dtype=float32)\n",
      "state_series_word1 Tensor(\"word_lstm1/Max:0\", shape=(10, 100), dtype=float32)\n",
      "state_series_w2 Tensor(\"word_lstm2/rnn_1/transpose_1:0\", shape=(10, 10, 100), dtype=float32)\n",
      "state_series_word2 Tensor(\"word_lstm2/Max:0\", shape=(10, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word_sz_sequence = 10\n",
    "n_units = 100\n",
    "n_layers = 3\n",
    "channels_word=100\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# with grap.as_default():\n",
    "#  For Word Embeddings\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"word_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, word_sz_sequence, channels_word))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_w1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_word[0])\n",
    "        state_series_word1 = tf.reduce_max(state_series_w1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_w1\", state_series_w1)\n",
    "        print(\"state_series_word1\", state_series_word1)\n",
    "    #  For Word Embeddings 2\n",
    "    with tf.variable_scope(\"word_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, word_sz_sequence, channels_word))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_w2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_word[1])\n",
    "        state_series_word2 = tf.reduce_max(state_series_w1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_w2\", state_series_w2)\n",
    "        print(\"state_series_word2\", state_series_word2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:50.711168Z",
     "start_time": "2020-11-28T05:15:48.741313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series_p1 Tensor(\"pos_lstm1/rnn_2/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_pos1 Tensor(\"pos_lstm1/Max:0\", shape=(10, 25), dtype=float32)\n",
      "state_series_w2 Tensor(\"pos_lstm2/rnn_3/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_pos2 Tensor(\"pos_lstm2/Max:0\", shape=(10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  For POS Embeddings 1\n",
    "pos_sz_sequence_POS = 10\n",
    "channels_POS = 25\n",
    "n_units_POS = 25\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"pos_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence_POS, channels_POS)) # 10X10X25\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_POS) for _ in range(n_layers)]\n",
    "        state_series_p1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_pos[0])\n",
    "        state_series_pos1 = tf.reduce_max(state_series_p1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_p1\", state_series_p1)\n",
    "        print(\"state_series_pos1\", state_series_pos1)\n",
    "\n",
    "    #  For POS Embeddings 2\n",
    "    with tf.variable_scope(\"pos_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence_POS, channels_POS)) # 10X25X3\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_POS) for _ in range(n_layers)]\n",
    "        state_series_p2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_pos[1])\n",
    "        state_series_pos2 = tf.reduce_max(state_series_p2, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_w2\", state_series_p2)\n",
    "        print(\"state_series_pos2\", state_series_pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T01:52:37.488221Z",
     "start_time": "2020-11-28T01:52:37.481999Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    my_scope = 'pos_lstm2'\n",
    "    scope_variables=  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=my_scope )\n",
    "    print(scope_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:55.668321Z",
     "start_time": "2020-11-28T05:15:54.287979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series_d1 Tensor(\"dep_lstm1/rnn_4/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_dep1 Tensor(\"dep_lstm1/Max:0\", shape=(10, 25), dtype=float32)\n",
      "state_series_d2 Tensor(\"dep_lstm2/rnn_5/transpose_1:0\", shape=(10, 10, 25), dtype=float32)\n",
      "state_series_dep2 Tensor(\"dep_lstm2/Max:0\", shape=(10, 25), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  For POS Embeddings 2\n",
    "dep_sz_sequence = 10\n",
    "channels_DEP = 25\n",
    "n_units_DEP = 25\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"dep_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, dep_sz_sequence, channels_DEP))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_DEP) for _ in range(n_layers)]\n",
    "        state_series_d1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_dep[0])\n",
    "        state_series_dep1 = tf.reduce_max(state_series_d1, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_d1\", state_series_d1)\n",
    "        print(\"state_series_dep1\", state_series_dep1)\n",
    "\n",
    "    with tf.variable_scope(\"dep_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, dep_sz_sequence, channels_DEP))#10X10X25\n",
    "        cells = [tf.keras.layers.GRUCell(n_units_DEP) for _ in range(n_layers)]\n",
    "        state_series_d2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(embedded_dep[1])\n",
    "        state_series_dep2 = tf.reduce_max(state_series_d2, axis=1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "        print(\"state_series_d2\", state_series_d2)\n",
    "        print(\"state_series_dep2\", state_series_dep2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:15:58.746023Z",
     "start_time": "2020-11-28T05:15:58.719164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series Tensor(\"concat_2:0\", shape=(10, 300), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "    state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)\n",
    "\n",
    "    state_series = tf.concat([state_series1, state_series2], 1)\n",
    "    init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "    print(\"state_series\", state_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:01.524958Z",
     "start_time": "2020-11-28T05:16:01.466407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"softmax_layer/add:0\", shape=(10, 19), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# grap = tf.Graph()\n",
    "with grap.as_default():\n",
    "    with tf.name_scope(\"hidden_layer\"):\n",
    "        W = tf.Variable(tf.truncated_normal([300, 100], -0.1, 0.1), name=\"W\")\n",
    "#         b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "        b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "        y_hidden_layer = tf.matmul(state_series, W) + b\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    with tf.name_scope(\"softmax_layer\"):\n",
    "        W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "        logits = tf.matmul(y_hidden_layer, W) + b\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T01:53:01.547172Z",
     "start_time": "2020-11-28T01:53:01.537962Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    my_scope = 'softmax_layer'\n",
    "    scope_variables=  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope= my_scope )\n",
    "    for scope_var in scope_variables:\n",
    "        print(scope_var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:10.940036Z",
     "start_time": "2020-11-28T05:16:10.922832Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    tv_all = tf.trainable_variables()\n",
    "    tv_regu = []\n",
    "    non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "    for t in tv_all:\n",
    "        if t.name not in non_reg:\n",
    "#             if(t.name.find('biases')==-1):\n",
    "            if(t.name.find('bias')==-1):\n",
    "                tv_regu.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:12:42.006375Z",
     "start_time": "2020-11-28T02:12:41.989239Z"
    }
   },
   "outputs": [],
   "source": [
    "# for tv in tv_regu:\n",
    "#     print(tv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:27.526547Z",
     "start_time": "2020-11-28T05:16:15.757093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        total_loss = loss + l2_loss\n",
    "        init=tf.global_variables_initializer()   # Initializing all Variables within the scope\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)\n",
    "    init=tf.global_variables_initializer()  # Initializing the optimizer variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:19:02.560065Z",
     "start_time": "2020-11-28T02:19:02.548667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'word_embedding/W:0' shape=(400001, 100) dtype=float32_ref>, <tf.Variable 'pos_embedding/W:0' shape=(10, 25) dtype=float32_ref>, <tf.Variable 'dep_embedding/W:0' shape=(21, 25) dtype=float32_ref>, <tf.Variable 'word_lstm1/rnn/kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/recurrent_kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/bias:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm1/rnn/kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/recurrent_kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/bias_1:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm1/rnn/kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/recurrent_kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm1/rnn/bias_2:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm1/rnn/Variable:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm1/rnn/Variable_1:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm1/rnn/Variable_2:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/recurrent_kernel:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/bias:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/recurrent_kernel_1:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/bias_1:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/recurrent_kernel_2:0' shape=(100, 300) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/bias_2:0' shape=(300,) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/Variable:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/Variable_1:0' shape=(10, 100) dtype=float32>, <tf.Variable 'word_lstm2/rnn_1/Variable_2:0' shape=(10, 100) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm1/rnn_2/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'pos_lstm2/rnn_3/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm1/rnn_4/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/recurrent_kernel:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/bias:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/recurrent_kernel_1:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/bias_1:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/recurrent_kernel_2:0' shape=(25, 75) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/bias_2:0' shape=(75,) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/Variable:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/Variable_1:0' shape=(10, 25) dtype=float32>, <tf.Variable 'dep_lstm2/rnn_5/Variable_2:0' shape=(10, 25) dtype=float32>, <tf.Variable 'hidden_layer/W:0' shape=(300, 100) dtype=float32_ref>, <tf.Variable 'hidden_layer/b:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'softmax_layer/W:0' shape=(100, 19) dtype=float32_ref>, <tf.Variable 'softmax_layer/b:0' shape=(19,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "### y, logits\n",
    "# tv_regu\n",
    "print(tv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:38.713250Z",
     "start_time": "2020-11-28T05:16:38.234064Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(data_dir + '/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token\n",
    "\n",
    "pos_tags_vocab = []\n",
    "for line in open(data_dir + '/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())\n",
    "\n",
    "dep_vocab = []\n",
    "for line in open(data_dir + '/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "relation_vocab = []\n",
    "for line in open(data_dir + '/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())\n",
    "\n",
    "\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'\n",
    "\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:42.421521Z",
     "start_time": "2020-11-28T05:16:42.400946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Component-Whole(e2,e1)', 'Other', 'Instrument-Agency(e2,e1)', 'Other', 'Member-Collection(e1,e2)', 'Other', 'Cause-Effect(e2,e1)', 'Entity-Destination(e1,e2)', 'Content-Container(e1,e2)', 'Entity-Destination(e1,e2)', 'Member-Collection(e1,e2)', 'Other', 'Message-Topic(e1,e2)', 'Cause-Effect(e2,e1)', 'Instrument-Agency(e2,e1)', 'Message-Topic(e1,e2)', 'Instrument-Agency(e2,e1)', 'Product-Producer(e2,e1)', 'Component-Whole(e2,e1)', 'Member-Collection(e2,e1)', 'Entity-Origin(e1,e2)', 'Member-Collection(e2,e1)', 'Cause-Effect(e1,e2)', 'Other', 'Member-Collection(e2,e1)', 'Other', 'Cause-Effect(e1,e2)', 'Message-Topic(e1,e2)', 'Message-Topic(e1,e2)', 'Component-Whole(e1,e2)', 'Message-Topic(e2,e1)', 'Cause-Effect(e2,e1)', 'Product-Producer(e1,e2)', 'Entity-Destination(e1,e2)', 'Component-Whole(e1,e2)', 'Entity-Origin(e1,e2)', 'Other', 'Component-Whole(e2,e1)', 'Cause-Effect(e1,e2)', 'Instrument-Agency(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "f = open(data_dir + '/train_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/train_relations.txt'):\n",
    "#     print(line.strip().split())\n",
    "    relations.append(line.strip().split()[0])\n",
    "print(relations)\n",
    "# rel2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:44.948912Z",
     "start_time": "2020-11-28T05:16:44.928782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Other': 0,\n",
       " 'Entity-Destination(e1,e2)': 1,\n",
       " 'Cause-Effect(e2,e1)': 2,\n",
       " 'Member-Collection(e2,e1)': 3,\n",
       " 'Entity-Origin(e1,e2)': 4,\n",
       " 'Message-Topic(e1,e2)': 5,\n",
       " 'Component-Whole(e2,e1)': 6,\n",
       " 'Component-Whole(e1,e2)': 7,\n",
       " 'Instrument-Agency(e2,e1)': 8,\n",
       " 'Product-Producer(e2,e1)': 9,\n",
       " 'Content-Container(e1,e2)': 10,\n",
       " 'Cause-Effect(e1,e2)': 11,\n",
       " 'Product-Producer(e1,e2)': 12,\n",
       " 'Content-Container(e2,e1)': 13,\n",
       " 'Entity-Origin(e2,e1)': 14,\n",
       " 'Message-Topic(e2,e1)': 15,\n",
       " 'Instrument-Agency(e1,e2)': 16,\n",
       " 'Member-Collection(e1,e2)': 17,\n",
       " 'Entity-Destination(e2,e1)': 18}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:35:13.431304Z",
     "start_time": "2020-11-26T21:35:13.415770Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_p1\n",
    "# max_len_path\n",
    "# word_p1_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:16:52.838833Z",
     "start_time": "2020-11-28T05:16:52.796548Z"
    }
   },
   "outputs": [],
   "source": [
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:25:03.129030Z",
     "start_time": "2020-11-28T05:25:03.110685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/checkpoint/modelv1/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "Load_Model_DIR = r\"/checkpoint/modelv1/\"\n",
    "Load_Model_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:35:28.290753Z",
     "start_time": "2020-11-28T05:35:28.271470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2397972.meta\n"
     ]
    }
   ],
   "source": [
    "aas=2397972\n",
    "print(\"{}.meta\".format(aas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:02:18.432612Z",
     "start_time": "2020-11-28T06:02:18.410437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\The University of Texas at Dallas\\\\Fall 2020\\\\Natural Language Processing- CS 6320.501\\\\NLP Project\\\\Relation-Classification-using-Bidirectional-LSTM-Tree\\\\Relation-Classification-using-Bidirectional-LSTM\\\\LCA Shortest Path\\\\checkpoint\\\\modelv1\\\\-440'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = r\"E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\LCA Shortest Path\\checkpoint\\modelv1\"\n",
    "checkpoint_file = tf.train.latest_checkpoint(DIR)\n",
    "checkpoint_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:00:15.435951Z",
     "start_time": "2020-11-28T06:00:05.835894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\LCA Shortest Path\\checkpoint\\modelv1\\-440\n"
     ]
    }
   ],
   "source": [
    "# Load_Model_DIR = r\"/checkpoint/modelv1/\"\n",
    "with grap.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        checkpoint_file = tf.train.latest_checkpoint(DIR)\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:28:02.654150Z",
     "start_time": "2020-11-28T02:28:02.640169Z"
    }
   },
   "outputs": [],
   "source": [
    "# saver\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     saver = tf.train.Saver(defer_build=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T23:27:09.566747Z",
     "start_time": "2020-11-27T23:27:09.560123Z"
    }
   },
   "outputs": [],
   "source": [
    "# with grap.as_default():\n",
    "#     print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dep_embedding'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T23:27:12.475231Z",
     "start_time": "2020-11-27T23:27:12.459848Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')\n",
    "with grap.as_default():\n",
    "    for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dep_embedding'):\n",
    "        print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:13:38.566364Z",
     "start_time": "2020-11-28T02:13:37.855854Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_dir = r\"E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\checkpoint\\modelv1\"\n",
    "model = tf.train.latest_checkpoint(model_DIR)\n",
    "saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T02:26:05.777263Z",
     "start_time": "2020-11-27T02:26:05.767701Z"
    }
   },
   "outputs": [],
   "source": [
    "# with grap.as_default():\n",
    "#     glob_var=  tf.get_collection(tf.GraphKeys.VARIABLES)\n",
    "# glob_var       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:19:54.332912Z",
     "start_time": "2020-11-28T02:19:54.314526Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(model_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:18:07.630226Z",
     "start_time": "2020-11-28T05:18:05.472490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "word_embedding/W:0 (float32_ref 400001x100) [40000100, bytes: 160000400]\n",
      "pos_embedding/W:0 (float32_ref 10x25) [250, bytes: 1000]\n",
      "dep_embedding/W:0 (float32_ref 21x25) [525, bytes: 2100]\n",
      "word_lstm1/rnn/kernel:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm1/rnn/recurrent_kernel:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm1/rnn/bias:0 (float32 300) [300, bytes: 1200]\n",
      "word_lstm1/rnn/kernel_1:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm1/rnn/recurrent_kernel_1:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm1/rnn/bias_1:0 (float32 300) [300, bytes: 1200]\n",
      "word_lstm1/rnn/kernel_2:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm1/rnn/recurrent_kernel_2:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm1/rnn/bias_2:0 (float32 300) [300, bytes: 1200]\n",
      "word_lstm1/rnn/Variable:0 (float32 10x100) [1000, bytes: 4000]\n",
      "word_lstm1/rnn/Variable_1:0 (float32 10x100) [1000, bytes: 4000]\n",
      "word_lstm1/rnn/Variable_2:0 (float32 10x100) [1000, bytes: 4000]\n",
      "word_lstm2/rnn_1/kernel:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm2/rnn_1/recurrent_kernel:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm2/rnn_1/bias:0 (float32 300) [300, bytes: 1200]\n",
      "word_lstm2/rnn_1/kernel_1:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm2/rnn_1/recurrent_kernel_1:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm2/rnn_1/bias_1:0 (float32 300) [300, bytes: 1200]\n",
      "word_lstm2/rnn_1/kernel_2:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm2/rnn_1/recurrent_kernel_2:0 (float32 100x300) [30000, bytes: 120000]\n",
      "word_lstm2/rnn_1/bias_2:0 (float32 300) [300, bytes: 1200]\n",
      "word_lstm2/rnn_1/Variable:0 (float32 10x100) [1000, bytes: 4000]\n",
      "word_lstm2/rnn_1/Variable_1:0 (float32 10x100) [1000, bytes: 4000]\n",
      "word_lstm2/rnn_1/Variable_2:0 (float32 10x100) [1000, bytes: 4000]\n",
      "pos_lstm1/rnn_2/kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm1/rnn_2/recurrent_kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm1/rnn_2/bias:0 (float32 75) [75, bytes: 300]\n",
      "pos_lstm1/rnn_2/kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm1/rnn_2/recurrent_kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm1/rnn_2/bias_1:0 (float32 75) [75, bytes: 300]\n",
      "pos_lstm1/rnn_2/kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm1/rnn_2/recurrent_kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm1/rnn_2/bias_2:0 (float32 75) [75, bytes: 300]\n",
      "pos_lstm1/rnn_2/Variable:0 (float32 10x25) [250, bytes: 1000]\n",
      "pos_lstm1/rnn_2/Variable_1:0 (float32 10x25) [250, bytes: 1000]\n",
      "pos_lstm1/rnn_2/Variable_2:0 (float32 10x25) [250, bytes: 1000]\n",
      "pos_lstm2/rnn_3/kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm2/rnn_3/recurrent_kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm2/rnn_3/bias:0 (float32 75) [75, bytes: 300]\n",
      "pos_lstm2/rnn_3/kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm2/rnn_3/recurrent_kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm2/rnn_3/bias_1:0 (float32 75) [75, bytes: 300]\n",
      "pos_lstm2/rnn_3/kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm2/rnn_3/recurrent_kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "pos_lstm2/rnn_3/bias_2:0 (float32 75) [75, bytes: 300]\n",
      "pos_lstm2/rnn_3/Variable:0 (float32 10x25) [250, bytes: 1000]\n",
      "pos_lstm2/rnn_3/Variable_1:0 (float32 10x25) [250, bytes: 1000]\n",
      "pos_lstm2/rnn_3/Variable_2:0 (float32 10x25) [250, bytes: 1000]\n",
      "dep_lstm1/rnn_4/kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm1/rnn_4/recurrent_kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm1/rnn_4/bias:0 (float32 75) [75, bytes: 300]\n",
      "dep_lstm1/rnn_4/kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm1/rnn_4/recurrent_kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm1/rnn_4/bias_1:0 (float32 75) [75, bytes: 300]\n",
      "dep_lstm1/rnn_4/kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm1/rnn_4/recurrent_kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm1/rnn_4/bias_2:0 (float32 75) [75, bytes: 300]\n",
      "dep_lstm1/rnn_4/Variable:0 (float32 10x25) [250, bytes: 1000]\n",
      "dep_lstm1/rnn_4/Variable_1:0 (float32 10x25) [250, bytes: 1000]\n",
      "dep_lstm1/rnn_4/Variable_2:0 (float32 10x25) [250, bytes: 1000]\n",
      "dep_lstm2/rnn_5/kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm2/rnn_5/recurrent_kernel:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm2/rnn_5/bias:0 (float32 75) [75, bytes: 300]\n",
      "dep_lstm2/rnn_5/kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm2/rnn_5/recurrent_kernel_1:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm2/rnn_5/bias_1:0 (float32 75) [75, bytes: 300]\n",
      "dep_lstm2/rnn_5/kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm2/rnn_5/recurrent_kernel_2:0 (float32 25x75) [1875, bytes: 7500]\n",
      "dep_lstm2/rnn_5/bias_2:0 (float32 75) [75, bytes: 300]\n",
      "dep_lstm2/rnn_5/Variable:0 (float32 10x25) [250, bytes: 1000]\n",
      "dep_lstm2/rnn_5/Variable_1:0 (float32 10x25) [250, bytes: 1000]\n",
      "dep_lstm2/rnn_5/Variable_2:0 (float32 10x25) [250, bytes: 1000]\n",
      "hidden_layer/W:0 (float32_ref 300x100) [30000, bytes: 120000]\n",
      "hidden_layer/b:0 (float32_ref 100) [100, bytes: 400]\n",
      "softmax_layer/W:0 (float32_ref 100x19) [1900, bytes: 7600]\n",
      "softmax_layer/b:0 (float32_ref 19) [19, bytes: 76]\n",
      "global_step:0 (int32_ref ) [1, bytes: 4]\n",
      "Total size of variables: 40449595\n",
      "Total bytes of variables: 161798380\n"
     ]
    }
   ],
   "source": [
    "# tf.convert_to_tensor(word_dict)\n",
    "# word_dict\n",
    "# grap\n",
    "import numpy as np\n",
    "from tensorflow.python.layers import base\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "with grap.as_default():\n",
    "    model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:20:02.191093Z",
     "start_time": "2020-11-28T02:20:02.103769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 80)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "        train_vars= tf.trainable_variables()\n",
    "        all_vars = tf.global_variables()\n",
    "len(all_vars), len(train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T01:55:26.852571Z",
     "start_time": "2020-11-28T01:55:26.830015Z"
    }
   },
   "outputs": [],
   "source": [
    "all_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T01:55:30.853830Z",
     "start_time": "2020-11-28T01:55:30.843084Z"
    }
   },
   "outputs": [],
   "source": [
    "train_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T23:27:49.603034Z",
     "start_time": "2020-11-27T23:27:49.586684Z"
    }
   },
   "outputs": [],
   "source": [
    "# with grap.as_default():\n",
    "#     with tf.Session() as sess:\n",
    "#         print(tf.global_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T08:14:23.487551Z",
     "start_time": "2020-11-23T08:14:23.461730Z"
    }
   },
   "outputs": [],
   "source": [
    "# path_dict\n",
    "# type(y_dict)\n",
    "# word_dict[0].shape\n",
    "# dep_dict[0].shape\n",
    "# path_dict[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:35:43.204713Z",
     "start_time": "2020-11-26T21:35:43.193218Z"
    }
   },
   "outputs": [],
   "source": [
    "# relations\n",
    "# sess = tf.Session()\n",
    "# sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T21:38:04.053688Z",
     "start_time": "2020-11-26T21:38:04.042918Z"
    }
   },
   "outputs": [],
   "source": [
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T01:35:55.524879Z",
     "start_time": "2020-11-27T01:35:51.691691Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.global_variables_initializer()\n",
    "# init_op = tf.initialize_all_variables()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# sess.run(init_op)\n",
    "# with grap.as_default():\n",
    "#     init_g = tf.global_variables_initializer()\n",
    "#     init_l = tf.local_variables_initializer()\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(init_g)\n",
    "#         sess.run(init_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:05:40.116864Z",
     "start_time": "2020-11-26T22:05:40.095760Z"
    }
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     tf.initialize_all_variables().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T22:11:29.652237Z",
     "start_time": "2020-11-26T22:11:29.637490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add an op to initialize the variables.\n",
    "# init_op = tf.initialize_all_variables()\n",
    "\n",
    "# Later, when launching the model\n",
    "# with tf.Session() as sess:\n",
    "#     print(sess.run(init_op))\n",
    "  # Run the init operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T02:27:50.615611Z",
     "start_time": "2020-11-27T02:27:48.435619Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    keras.backend.get_session().run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T02:28:13.002178Z",
     "start_time": "2020-11-27T02:28:12.989255Z"
    }
   },
   "outputs": [],
   "source": [
    "# with grap.as_default():\n",
    "tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:41:34.777622Z",
     "start_time": "2020-11-28T02:41:34.765692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\The University of Texas at Dallas\\\\Fall 2020\\\\Natural Language Processing- CS 6320.501\\\\NLP Project\\\\Relation-Classification-using-Bidirectional-LSTM-Tree\\\\Relation-Classification-using-Bidirectional-LSTM\\\\LCA Shortest Path'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:09:57.290667Z",
     "start_time": "2020-11-28T02:09:57.276810Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_dir + '/model'\n",
    "# model_DIR = r\"checkpoint/modelv1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:38:35.637411Z",
     "start_time": "2020-11-28T02:36:25.136700Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 50 loss: 1.3380758\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 60 loss: 1.461537\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 70 loss: 0.8698428\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 80 loss: 0.9936742\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 90 loss: 0.64139134\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 100 loss: 0.65540016\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 110 loss: 0.41867268\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 120 loss: 0.42527044\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 130 loss: 0.23097923\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 140 loss: 0.2554982\n",
      "WARNING:tensorflow:From C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 150 loss: 0.14061832\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 160 loss: 0.16118369\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 170 loss: 0.11110988\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 180 loss: 0.11916461\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 190 loss: 0.09847464\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 200 loss: 0.10279262\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 210 loss: 0.09288431\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 220 loss: 0.095011346\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 230 loss: 0.08905603\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 240 loss: 0.09067411\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 250 loss: 0.08606377\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 260 loss: 0.08757415\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 270 loss: 0.083871186\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 280 loss: 0.08503239\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 290 loss: 0.08204543\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 300 loss: 0.08294806\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 310 loss: 0.08039717\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 320 loss: 0.08112839\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 330 loss: 0.07883621\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 340 loss: 0.079486825\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 350 loss: 0.07737551\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 360 loss: 0.07800338\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 370 loss: 0.07603583\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 380 loss: 0.07661753\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 390 loss: 0.07477363\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 400 loss: 0.075349845\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 410 loss: 0.07358013\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 420 loss: 0.074190475\n",
      "Saved Model at checkpoint/modelv1/\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 430 loss: 0.07245664\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Sess Running\n",
      "Step: 440 loss: 0.07304098\n",
      "Saved Model at checkpoint/modelv1/\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "        word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "#         path_dict = tf.convert_to_tensor(path_dict)\n",
    "#         word_dict = tf.convert_to_tensor(word_dict)\n",
    "#         pos_dict = tf.convert_to_tensor(pos_dict)\n",
    "#         dep_dict = tf.convert_to_tensor(dep_dict)\n",
    "#         y_dict = tf.convert_to_tensor(y_dict)\n",
    "        \n",
    "        feed_dict = {\n",
    "            path_length:path_dict,\n",
    "            word_ids:word_dict,\n",
    "            pos_ids:pos_dict,\n",
    "            dep_ids:dep_dict,\n",
    "            y:y_dict}\n",
    "        with grap.as_default():\n",
    "#             with tf.Session(graph=grap) as sess:\n",
    "            print(\"Sess Running\")\n",
    "            _, loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "            if step%10==0:\n",
    "                print(\"Step:\", step, \"loss:\",loss)\n",
    "            if step % 20 == 0:\n",
    "                saver.save(sess, model_DIR , global_step=step)\n",
    "                print(\"Saved Model at\", model_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:35:23.330044Z",
     "start_time": "2020-11-28T02:35:23.316132Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint/modelv1/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_DIR = r\"checkpoint/modelv1/\"\n",
    "model_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with grap.as_default():\n",
    "#         if i % 5 == 0:\n",
    "#             saver.save(sess, model_dir + '/model')\n",
    "#             print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:39:28.556251Z",
     "start_time": "2020-11-28T02:39:28.547074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:39:34.059053Z",
     "start_time": "2020-11-28T02:39:33.487710Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sess Running for predictions\n",
      "Sess Running for predictions\n",
      "Sess Running for predictions\n",
      "Sess Running for predictions\n",
      "training accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "# training accuracy\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    with grap.as_default():\n",
    "        batch_predictions = sess.run(predictions, feed_dict)\n",
    "        print(\"Sess Running for predictions\")\n",
    "        all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"training accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:20:56.300971Z",
     "start_time": "2020-11-28T05:20:55.965949Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(data_dir + '/test_pathsv1', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/test_relations.txt'):\n",
    "    relations.append(line.strip().split()[0])\n",
    "\n",
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:44:24.703587Z",
     "start_time": "2020-11-28T02:44:24.701306Z"
    }
   },
   "outputs": [],
   "source": [
    "# word_p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:47:13.292627Z",
     "start_time": "2020-11-28T05:47:13.272807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  0,  8, ..., 16, 13,  6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:47:15.490261Z",
     "start_time": "2020-11-28T05:47:15.475109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2717"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_dict\n",
    "len(dep_p1_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:05:06.221772Z",
     "start_time": "2020-11-28T06:04:46.297890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\LCA Shortest Path\\checkpoint\\modelv1\\-440\n",
      "test accuracy 6.051660516605166\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "with grap.as_default():\n",
    "    with tf.Session() as sess1:\n",
    "        checkpoint_file = tf.train.latest_checkpoint(DIR)\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess1, checkpoint_file)\n",
    "        all_predictions = []\n",
    "        for j in range(num_batches):\n",
    "            path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "            word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "            pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "            dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "            y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "            feed_dict = {\n",
    "                path_length:path_dict,\n",
    "                word_ids:word_dict,\n",
    "                pos_ids:pos_dict,\n",
    "                dep_ids:dep_dict,\n",
    "                y:y_dict}\n",
    "#             with grap.as_default():\n",
    "#                 #     with tf.Session() as sess:\n",
    "            batch_predictions = sess1.run(predictions, feed_dict)\n",
    "            all_predictions.append(batch_predictions)\n",
    "\n",
    "        y_pred = []\n",
    "        for i in range(num_batches):\n",
    "            for pred in all_predictions[i]:\n",
    "                y_pred.append(pred)\n",
    "\n",
    "        count = 0\n",
    "        for i in range(batch_size*num_batches):\n",
    "            count += y_pred[i]==rel_ids[i]\n",
    "        accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "        print(\"test accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:09:53.947583Z",
     "start_time": "2020-11-28T06:09:53.938425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  8, 11,  0,  3, 11, 11, 12, 12,  7], dtype=int64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T02:47:16.236390Z",
     "start_time": "2020-11-28T02:47:16.223797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Other': 0,\n",
       " 'Entity-Destination(e1,e2)': 1,\n",
       " 'Cause-Effect(e2,e1)': 2,\n",
       " 'Member-Collection(e2,e1)': 3,\n",
       " 'Entity-Origin(e1,e2)': 4,\n",
       " 'Message-Topic(e1,e2)': 5,\n",
       " 'Component-Whole(e2,e1)': 6,\n",
       " 'Component-Whole(e1,e2)': 7,\n",
       " 'Instrument-Agency(e2,e1)': 8,\n",
       " 'Product-Producer(e2,e1)': 9,\n",
       " 'Content-Container(e1,e2)': 10,\n",
       " 'Cause-Effect(e1,e2)': 11,\n",
       " 'Product-Producer(e1,e2)': 12,\n",
       " 'Content-Container(e2,e1)': 13,\n",
       " 'Entity-Origin(e2,e1)': 14,\n",
       " 'Message-Topic(e2,e1)': 15,\n",
       " 'Instrument-Agency(e1,e2)': 16,\n",
       " 'Member-Collection(e1,e2)': 17,\n",
       " 'Entity-Destination(e2,e1)': 18}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:10:07.666526Z",
     "start_time": "2020-11-28T06:10:07.648586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Other', 1: 'Entity-Destination(e1,e2)', 2: 'Cause-Effect(e2,e1)', 3: 'Member-Collection(e2,e1)', 4: 'Entity-Origin(e1,e2)', 5: 'Message-Topic(e1,e2)', 6: 'Component-Whole(e2,e1)', 7: 'Component-Whole(e1,e2)', 8: 'Instrument-Agency(e2,e1)', 9: 'Product-Producer(e2,e1)', 10: 'Content-Container(e1,e2)', 11: 'Cause-Effect(e1,e2)', 12: 'Product-Producer(e1,e2)', 13: 'Content-Container(e2,e1)', 14: 'Entity-Origin(e2,e1)', 15: 'Message-Topic(e2,e1)', 16: 'Instrument-Agency(e1,e2)', 17: 'Member-Collection(e1,e2)', 18: 'Entity-Destination(e2,e1)'}\n"
     ]
    }
   ],
   "source": [
    "ID2REL = {v: k for k, v in rel2id.items()}\n",
    "print(ID2REL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T06:10:11.807360Z",
     "start_time": "2020-11-28T06:10:11.793033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Member-Collection(e1,e2)\n",
      "Instrument-Agency(e2,e1)\n",
      "Cause-Effect(e1,e2)\n",
      "Instrument-Agency(e2,e1)\n",
      "Member-Collection(e2,e1)\n",
      "Cause-Effect(e1,e2)\n",
      "Cause-Effect(e1,e2)\n",
      "Product-Producer(e1,e2)\n",
      "Product-Producer(e1,e2)\n",
      "Component-Whole(e1,e2)\n"
     ]
    }
   ],
   "source": [
    "for pred in batch_predictions:\n",
    "    print(ID2REL[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T05:48:57.515432Z",
     "start_time": "2020-11-28T05:48:57.503296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x283f385d088>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
