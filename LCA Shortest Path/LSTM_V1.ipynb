{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:03.628160Z",
     "start_time": "2020-11-21T09:22:54.805088Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, _pickle as pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:04.736641Z",
     "start_time": "2020-11-21T09:23:04.719516Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'                        # Directory for Data and Other files\n",
    "ckpt_dir = '../checkpoint'                  # Directory for Checkpoints \n",
    "word_embd_dir = '../checkpoint/word_embd'   # Directory for Checkpoints of Word Embedding Layer\n",
    "model_dir = '../checkpoint/modelv1'         # Directory for Checkpoints of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:07.991264Z",
     "start_time": "2020-11-21T09:23:07.974834Z"
    }
   },
   "outputs": [],
   "source": [
    "word_embd_dim = 100       # Dimension of embedding layer for words\n",
    "pos_embd_dim = 25         # Dimension of embedding layer for POS Tags\n",
    "dep_embd_dim = 25         # Dimension of embedding layer for Dependency Types\n",
    "\n",
    "word_vocab_size = 400001  # Vocab size for Words\n",
    "pos_vocab_size = 10       # Vocab size for POS Tags\n",
    "dep_vocab_size = 21       # Vocab size for Dependency Types\n",
    "word_state_size = 100\n",
    "other_state_size = 100\n",
    "relation_classes = 19     # No. of Relation Classes\n",
    "state_size = 100          # Dimension of States of LSTM-RNNs\n",
    "batch_size = 10           # Batch Size for training\n",
    "\n",
    "channels = 3              # No. of types of features to feed in LSTM-RNN\n",
    "lambda_l2 = 0.0001\n",
    "max_len_path = 10         # Maximum length of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:10.496641Z",
     "start_time": "2020-11-21T09:23:10.449424Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "grap = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:12.187149Z",
     "start_time": "2020-11-21T09:23:12.127818Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    with tf.name_scope(\"input\"):\n",
    "    \n",
    "    # Length of the sequence\n",
    "        path_length = tf.placeholder(tf.int32, shape=[2, batch_size], name=\"path1_length\") \n",
    "\n",
    "        # Words in the sequence\n",
    "        word_ids = tf.placeholder(tf.int32, shape=[2, batch_size, max_len_path], name=\"word_ids\") \n",
    "\n",
    "         # POS Tags in the sequence\n",
    "        pos_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"pos_ids\") \n",
    "\n",
    "        # Dependency Types in the sequence\n",
    "        dep_ids = tf.placeholder(tf.int32, [2, batch_size, max_len_path], name=\"dep_ids\") \n",
    "\n",
    "         # True Relation btw the entities\n",
    "        y = tf.placeholder(tf.int32, [batch_size], name=\"y\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:13.689796Z",
     "start_time": "2020-11-21T09:23:13.632890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'input/word_ids:0' shape=(2, 10, 10) dtype=int32>,\n",
       " <tf.Tensor 'input/dep_ids:0' shape=(2, 10, 10) dtype=int32>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids, dep_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:37.216896Z",
     "start_time": "2020-11-21T09:23:37.103239Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embedding Layer of Words \n",
    "with grap.as_default():\n",
    "    with tf.name_scope(\"word_embedding\"):\n",
    "        W = tf.Variable(tf.constant(0.0, shape=[word_vocab_size, word_embd_dim]), name=\"W\")\n",
    "        embedding_placeholder = tf.placeholder(tf.float32,[word_vocab_size, word_embd_dim])\n",
    "        embedding_init = W.assign(embedding_placeholder)\n",
    "        embedded_word = tf.nn.embedding_lookup(W, word_ids)\n",
    "        word_embedding_saver = tf.train.Saver({\"word_embedding/W\": W})\n",
    "\n",
    "    # Embedding Layer of POS Tags \n",
    "    with tf.name_scope(\"pos_embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([pos_vocab_size, pos_embd_dim]), name=\"W\")\n",
    "        embedded_pos = tf.nn.embedding_lookup(W, pos_ids)\n",
    "        pos_embedding_saver = tf.train.Saver({\"pos_embedding/W\": W})\n",
    "\n",
    "    # Embedding Layer of Dependency Types \n",
    "    with tf.name_scope(\"dep_embedding\"):\n",
    "        W = tf.Variable(tf.random_uniform([dep_vocab_size, dep_embd_dim]), name=\"W\")\n",
    "        embedded_dep = tf.nn.embedding_lookup(W, dep_ids)\n",
    "        dep_embedding_saver = tf.train.Saver({\"dep_embedding/W\": W})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:39.305792Z",
     "start_time": "2020-11-21T09:23:39.285935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'word_embedding/embedding_lookup/Identity:0' shape=(2, 10, 10, 100) dtype=float32>,\n",
       " <tf.Variable 'dep_embedding/W:0' shape=(21, 25) dtype=float32_ref>,\n",
       " <tf.Tensor 'dep_embedding/embedding_lookup/Identity:0' shape=(2, 10, 10, 25) dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_word, W, embedded_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:41.598478Z",
     "start_time": "2020-11-21T09:23:41.589506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(embedded_word)\n",
    "# with tf.Session() as ss:\n",
    "#     print(embedded_word.eval())\n",
    "# tf.Print(embedded_word)\n",
    "type(embedded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:15:00.277941Z",
     "start_time": "2020-11-19T07:15:00.273700Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:44.137535Z",
     "start_time": "2020-11-21T09:23:44.122830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow._api.v1.version' from 'C:\\\\Users\\\\sadam\\\\Anaconda3\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v1\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:23:27.510297Z",
     "start_time": "2020-11-20T07:23:27.504242Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:22:25.876899Z",
     "start_time": "2020-11-20T07:22:25.292371Z"
    }
   },
   "outputs": [],
   "source": [
    "# sz_sequence = 100\n",
    "# n_units = 120\n",
    "# n_layers = 3\n",
    "# with tf.variable_scope(\"word_lstm1\"):\n",
    "#     inputs = tf.keras.layers.Input(batch_shape=(batch_size, sz_sequence, channels))\n",
    "#     cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "#     state_series = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)\n",
    "    \n",
    "    \n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=init_states[0])\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:49:20.056735Z",
     "start_time": "2020-11-20T07:49:19.976990Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a BasicRNNCell\n",
    "# rnn_cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "# # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "\n",
    "# # defining initial state\n",
    "# initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "# # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "# outputs, state = tf.compat.v1.nn.dynamic_rnn(rnn_cell, embedded_word[0],\n",
    "#                                    initial_state=initial_state,\n",
    "#                                    dtype=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating LSTM Layers for Different Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T08:10:43.314330Z",
     "start_time": "2020-11-20T08:10:42.953429Z"
    }
   },
   "outputs": [],
   "source": [
    "# sz_sequence = 10\n",
    "# n_units = 100\n",
    "# n_layers = 3\n",
    "# tf.reset_default_graph()\n",
    "# #  For Word Embeddings\n",
    "# with tf.variable_scope(\"word_lstm1\"):\n",
    "#     inputs = tf.keras.layers.Input(batch_shape=(batch_size, sz_sequence, channels))\n",
    "#     cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "#     state_series = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T08:10:45.630280Z",
     "start_time": "2020-11-20T08:10:45.613887Z"
    }
   },
   "outputs": [],
   "source": [
    "state_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:23:53.602289Z",
     "start_time": "2020-11-21T09:23:51.496938Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "state_series_w1 Tensor(\"word_lstm1/rnn/transpose_1:0\", shape=(10, 100, 100), dtype=float32)\n",
      "state_series_word1 Tensor(\"word_lstm1/Max:0\", shape=(10, 100), dtype=float32)\n",
      "state_series_w2 Tensor(\"word_lstm2/rnn_1/transpose_1:0\", shape=(10, 100, 100), dtype=float32)\n",
      "state_series_word2 Tensor(\"word_lstm2/Max:0\", shape=(10, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word_sz_sequence = 100\n",
    "n_units = 100\n",
    "n_layers = 3\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# with grap.as_default():\n",
    "#  For Word Embeddings\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"word_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, word_sz_sequence, channels))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_w1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "        state_series_word1 = tf.reduce_max(state_series_w1, axis=1)\n",
    "        print(\"state_series_w1\", state_series_w1)\n",
    "        print(\"state_series_word1\", state_series_word1)\n",
    "    #  For Word Embeddings 2\n",
    "    with tf.variable_scope(\"word_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, word_sz_sequence, channels))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_w2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "        state_series_word2 = tf.reduce_max(state_series_w1, axis=1)\n",
    "        print(\"state_series_w2\", state_series_w2)\n",
    "        print(\"state_series_word2\", state_series_word2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:00.348442Z",
     "start_time": "2020-11-21T09:23:58.089519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series_p1 Tensor(\"pos_lstm1/rnn_2/transpose_1:0\", shape=(10, 25, 100), dtype=float32)\n",
      "state_series_pos1 Tensor(\"pos_lstm1/Max:0\", shape=(10, 100), dtype=float32)\n",
      "state_series_w2 Tensor(\"word_lstm2/rnn_1/transpose_1:0\", shape=(10, 100, 100), dtype=float32)\n",
      "state_series_pos2 Tensor(\"pos_lstm2/Max:0\", shape=(10, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  For POS Embeddings 1\n",
    "pos_sz_sequence = 25\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"pos_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence, channels))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_p1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "        state_series_pos1 = tf.reduce_max(state_series_p1, axis=1)\n",
    "        print(\"state_series_p1\", state_series_p1)\n",
    "        print(\"state_series_pos1\", state_series_pos1)\n",
    "\n",
    "    #  For POS Embeddings 2\n",
    "    with tf.variable_scope(\"pos_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence, channels))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_p2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "        state_series_pos2 = tf.reduce_max(state_series_p2, axis=1)\n",
    "        print(\"state_series_w2\", state_series_w2)\n",
    "        print(\"state_series_pos2\", state_series_pos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:04.824757Z",
     "start_time": "2020-11-21T09:24:03.005884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series_d1 Tensor(\"dep_lstm1/rnn_4/transpose_1:0\", shape=(10, 25, 100), dtype=float32)\n",
      "state_series_dep1 Tensor(\"dep_lstm1/Max:0\", shape=(10, 100), dtype=float32)\n",
      "state_series_d2 Tensor(\"dep_lstm2/rnn_5/transpose_1:0\", shape=(10, 25, 100), dtype=float32)\n",
      "state_series_dep2 Tensor(\"dep_lstm2/Max:0\", shape=(10, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  For POS Embeddings 2\n",
    "dep_sz_sequence = 25\n",
    "with grap.as_default():\n",
    "    with tf.variable_scope(\"dep_lstm1\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, dep_sz_sequence, channels))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_d1 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "        state_series_dep1 = tf.reduce_max(state_series_d1, axis=1)\n",
    "        print(\"state_series_d1\", state_series_d1)\n",
    "        print(\"state_series_dep1\", state_series_dep1)\n",
    "\n",
    "    with tf.variable_scope(\"dep_lstm2\"):\n",
    "        inputs = tf.keras.layers.Input(batch_shape=(batch_size, dep_sz_sequence, channels))\n",
    "        cells = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "        state_series_d2 = tf.keras.layers.RNN(cells, stateful=True, return_sequences=True, return_state=False)(inputs)\n",
    "        state_series_dep2 = tf.reduce_max(state_series_d2, axis=1)\n",
    "        print(\"state_series_d2\", state_series_d2)\n",
    "        print(\"state_series_dep2\", state_series_dep2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:58:56.385127Z",
     "start_time": "2020-11-21T09:58:56.364902Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.recurrent.GRUCell at 0x1e201f59308>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce = [tf.keras.layers.GRUCell(n_units) for _ in range(n_layers)]\n",
    "ce[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:06.143561Z",
     "start_time": "2020-11-21T09:24:06.118171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_series Tensor(\"concat_2:0\", shape=(10, 600), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "    state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)\n",
    "\n",
    "    state_series = tf.concat([state_series1, state_series2], 1)\n",
    "    print(\"state_series\", state_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated RNN-LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T09:40:29.653977Z",
     "start_time": "2020-11-19T09:40:29.577049Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# hidden_states = tf.zeros([channels, batch_size, state_size], name=\"hidden_state\")\n",
    "# cell_states = tf.zeros([channels, batch_size, state_size], name=\"cell_state\")\n",
    "\n",
    "# init_states = [tf.contrib.rnn.LSTMStateTuple(hidden_states[i], cell_states[i]) for i in range(channels)]\n",
    "# tf.reset_default_graph()\n",
    "# #  For Word Embeddings\n",
    "# with tf.variable_scope(\"word_lstm1\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[0], sequence_length=path_length[0], initial_state=init_states[0])\n",
    "#     state_series_word1 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For Word Embeddings 2\n",
    "# with tf.variable_scope(\"word_lstm2\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(word_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_word[1], sequence_length=path_length[1], initial_state=init_states[0])\n",
    "#     state_series_word2 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For POS Embeddings 1\n",
    "# with tf.variable_scope(\"pos_lstm1\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[0], sequence_length=path_length[0],initial_state=init_states[1])\n",
    "#     state_series_pos1 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For POS Embeddings 2\n",
    "# with tf.variable_scope(\"pos_lstm2\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_pos[1], sequence_length=path_length[1],initial_state=init_states[1])\n",
    "#     state_series_pos2 = tf.reduce_max(state_series, axis=1)\n",
    "# #  For POS Embeddings 2\n",
    "# with tf.variable_scope(\"dep_lstm1\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[0], sequence_length=path_length[0], initial_state=init_states[2])\n",
    "#     state_series_dep1 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "# with tf.variable_scope(\"dep_lstm2\"):\n",
    "#     cell = tf.contrib.rnn.BasicLSTMCell(other_state_size)\n",
    "#     state_series, current_state = tf.nn.dynamic_rnn(cell, embedded_dep[1], sequence_length=path_length[1], initial_state=init_states[2])\n",
    "#     state_series_dep2 = tf.reduce_max(state_series, axis=1)\n",
    "\n",
    "# state_series1 = tf.concat([state_series_word1, state_series_pos1, state_series_dep1], 1)\n",
    "# state_series2 = tf.concat([state_series_word2, state_series_pos2, state_series_dep2], 1)\n",
    "\n",
    "# state_series = tf.concat([state_series1, state_series2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:19.567714Z",
     "start_time": "2020-11-21T09:24:19.541748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'strided_slice:0' shape=(10, 10, 25) dtype=float32>,\n",
       " <tf.Tensor 'word_embedding/embedding_lookup/Identity:0' shape=(2, 10, 10, 100) dtype=float32>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_dep[0], embedded_word\n",
    "# tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:21.702277Z",
     "start_time": "2020-11-21T09:24:21.633508Z"
    }
   },
   "outputs": [],
   "source": [
    "# grap = tf.Graph()\n",
    "with grap.as_default():\n",
    "    with tf.name_scope(\"hidden_layer\"):\n",
    "        W = tf.Variable(tf.truncated_normal([600, 100], -0.1, 0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([100]), name=\"b\")\n",
    "        y_hidden_layer = tf.matmul(state_series, W) + b\n",
    "\n",
    "    with tf.name_scope(\"softmax_layer\"):\n",
    "        W = tf.Variable(tf.truncated_normal([100, relation_classes], -0.1, 0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([relation_classes]), name=\"b\")\n",
    "        logits = tf.matmul(y_hidden_layer, W) + b\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     # True Relation btw the entities\n",
    "#     y = tf.placeholder(tf.int32, [batch_size], name=\"y\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:24.157781Z",
     "start_time": "2020-11-21T09:24:24.141608Z"
    }
   },
   "outputs": [],
   "source": [
    "with grap.as_default():\n",
    "    tv_all = tf.trainable_variables()\n",
    "    tv_regu = []\n",
    "    non_reg = [\"word_embedding/W:0\",\"pos_embedding/W:0\",'dep_embedding/W:0',\"global_step:0\",'hidden_layer/b:0','softmax_layer/b:0']\n",
    "    for t in tv_all:\n",
    "        if t.name not in non_reg:\n",
    "            if(t.name.find('biases')==-1):\n",
    "                tv_regu.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:27.902060Z",
     "start_time": "2020-11-21T09:24:27.892219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'input/y:0' shape=(10,) dtype=int32>,\n",
       " <tf.Tensor 'softmax_layer/add:0' shape=(10, 19) dtype=float32>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:41.906162Z",
     "start_time": "2020-11-21T09:24:30.411604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "with grap.as_default():\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        l2_loss = lambda_l2 * tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv_regu ])\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        total_loss = loss + l2_loss\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\")\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(total_loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:47.992519Z",
     "start_time": "2020-11-21T09:24:47.483398Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(data_dir + '/vocab.pkl', 'rb')\n",
    "vocab = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "word2id = dict((w, i) for i,w in enumerate(vocab))\n",
    "id2word = dict((i, w) for i,w in enumerate(vocab))\n",
    "\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "word2id[unknown_token] = word_vocab_size -1\n",
    "id2word[word_vocab_size-1] = unknown_token\n",
    "\n",
    "pos_tags_vocab = []\n",
    "for line in open(data_dir + '/pos_tags.txt'):\n",
    "        pos_tags_vocab.append(line.strip())\n",
    "\n",
    "dep_vocab = []\n",
    "for line in open(data_dir + '/dependency_types.txt'):\n",
    "    dep_vocab.append(line.strip())\n",
    "\n",
    "relation_vocab = []\n",
    "for line in open(data_dir + '/relation_types.txt'):\n",
    "    relation_vocab.append(line.strip())\n",
    "\n",
    "\n",
    "rel2id = dict((w, i) for i,w in enumerate(relation_vocab))\n",
    "id2rel = dict((i, w) for i,w in enumerate(relation_vocab))\n",
    "\n",
    "pos_tag2id = dict((w, i) for i,w in enumerate(pos_tags_vocab))\n",
    "id2pos_tag = dict((i, w) for i,w in enumerate(pos_tags_vocab))\n",
    "\n",
    "dep2id = dict((w, i) for i,w in enumerate(dep_vocab))\n",
    "id2dep = dict((i, w) for i,w in enumerate(dep_vocab))\n",
    "\n",
    "pos_tag2id['OTH'] = 9\n",
    "id2pos_tag[9] = 'OTH'\n",
    "\n",
    "dep2id['OTH'] = 20\n",
    "id2dep[20] = 'OTH'\n",
    "\n",
    "JJ_pos_tags = ['JJ', 'JJR', 'JJS']\n",
    "NN_pos_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "RB_pos_tags = ['RB', 'RBR', 'RBS']\n",
    "PRP_pos_tags = ['PRP', 'PRP$']\n",
    "VB_pos_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "_pos_tags = ['CC', 'CD', 'DT', 'IN']\n",
    "\n",
    "def pos_tag(x):\n",
    "    if x in JJ_pos_tags:\n",
    "        return pos_tag2id['JJ']\n",
    "    if x in NN_pos_tags:\n",
    "        return pos_tag2id['NN']\n",
    "    if x in RB_pos_tags:\n",
    "        return pos_tag2id['RB']\n",
    "    if x in PRP_pos_tags:\n",
    "        return pos_tag2id['PRP']\n",
    "    if x in VB_pos_tags:\n",
    "        return pos_tag2id['VB']\n",
    "    if x in _pos_tags:\n",
    "        return pos_tag2id[x]\n",
    "    else:\n",
    "        return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:54.109670Z",
     "start_time": "2020-11-21T09:24:54.054160Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open(data_dir + '/train_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/train_relations.txt'):\n",
    "#     print(line.strip().split())\n",
    "    relations.append(line.strip().split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:24:55.782942Z",
     "start_time": "2020-11-21T09:24:55.775725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Component-Whole(e2,e1)', 'Other', 'Instrument-Agency(e2,e1)', 'Other', 'Member-Collection(e1,e2)', 'Other', 'Cause-Effect(e2,e1)', 'Entity-Destination(e1,e2)', 'Content-Container(e1,e2)', 'Entity-Destination(e1,e2)', 'Member-Collection(e1,e2)', 'Other', 'Message-Topic(e1,e2)', 'Cause-Effect(e2,e1)', 'Instrument-Agency(e2,e1)', 'Message-Topic(e1,e2)', 'Instrument-Agency(e2,e1)', 'Product-Producer(e2,e1)', 'Component-Whole(e2,e1)', 'Member-Collection(e2,e1)', 'Entity-Origin(e1,e2)', 'Member-Collection(e2,e1)', 'Cause-Effect(e1,e2)', 'Other', 'Member-Collection(e2,e1)', 'Other', 'Cause-Effect(e1,e2)', 'Message-Topic(e1,e2)', 'Message-Topic(e1,e2)', 'Component-Whole(e1,e2)', 'Message-Topic(e2,e1)', 'Cause-Effect(e2,e1)', 'Product-Producer(e1,e2)', 'Entity-Destination(e1,e2)', 'Component-Whole(e1,e2)', 'Entity-Origin(e1,e2)', 'Other', 'Component-Whole(e2,e1)', 'Cause-Effect(e1,e2)', 'Instrument-Agency(e2,e1)']\n"
     ]
    }
   ],
   "source": [
    "print(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:25:00.068296Z",
     "start_time": "2020-11-21T09:25:00.025327Z"
    }
   },
   "outputs": [],
   "source": [
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:28:55.279623Z",
     "start_time": "2020-11-21T09:28:55.262123Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(defer_build=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('data/word_embedding', 'rb')\n",
    "# word_embedding = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# sess.run(embedding_init, feed_dict={embedding_placeholder:word_embedding})\n",
    "# word_embedding_saver.save(sess, word_embd_dir + '/word_embd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:28:59.282187Z",
     "start_time": "2020-11-21T09:28:59.245310Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0d26ceb57066>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model_dir = r\"E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\checkpoint\\modelv1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1272\u001b[0m       \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheckpoint_management\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "# model_dir = r\"E:\\The University of Texas at Dallas\\Fall 2020\\Natural Language Processing- CS 6320.501\\NLP Project\\Relation-Classification-using-Bidirectional-LSTM-Tree\\Relation-Classification-using-Bidirectional-LSTM\\checkpoint\\modelv1\"\n",
    "model = tf.train.latest_checkpoint(model_dir)\n",
    "saver.restore(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T09:30:10.107101Z",
     "start_time": "2020-11-20T09:30:10.097633Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:29:45.209878Z",
     "start_time": "2020-11-21T09:29:45.198103Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../checkpoint/modelv1'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# latest_embd = tf.train.latest_checkpoint(word_embd_dir)\n",
    "# word_embedding_saver.restore(sess, latest_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:29:51.554040Z",
     "start_time": "2020-11-21T09:29:51.533865Z"
    }
   },
   "outputs": [],
   "source": [
    "# grap.version\n",
    "from keras import backend as K\n",
    "\n",
    "#Before prediction\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:39:28.336640Z",
     "start_time": "2020-11-21T09:39:28.323359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.ops.Graph at 0x1e26f9c60c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.convert_to_tensor(word_dict)\n",
    "# word_dict\n",
    "grap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:56:58.768982Z",
     "start_time": "2020-11-21T09:56:58.751528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_dict\n",
    "# type(y_dict)\n",
    "# word_dict[0].shape\n",
    "dep_dict[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-21T09:50:22.942424Z",
     "start_time": "2020-11-21T09:50:16.252212Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'pos_lstm1/input_3' with dtype float and shape [10,25,3]\n\t [[node pos_lstm1/input_3 (defined at <ipython-input-13-c96a2c1911c4>:5) ]]\n\nOriginal stack trace for 'pos_lstm1/input_3':\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-c96a2c1911c4>\", line 5, in <module>\n    inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence, channels))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\", line 248, in Input\n    input_tensor=tensor)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\", line 123, in __init__\n    name=self.name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 998, in placeholder\n    x = array_ops.placeholder(dtype, shape=shape, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7400, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'pos_lstm1/input_3' with dtype float and shape [10,25,3]\n\t [[{{node pos_lstm1/input_3}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-b43e4cc958ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             y:y_dict}\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrap\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'pos_lstm1/input_3' with dtype float and shape [10,25,3]\n\t [[node pos_lstm1/input_3 (defined at <ipython-input-13-c96a2c1911c4>:5) ]]\n\nOriginal stack trace for 'pos_lstm1/input_3':\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 272, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2855, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3058, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3249, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-c96a2c1911c4>\", line 5, in <module>\n    inputs = tf.keras.layers.Input(batch_shape=(batch_size, pos_sz_sequence, channels))\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\", line 248, in Input\n    input_tensor=tensor)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_layer.py\", line 123, in __init__\n    name=self.name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 998, in placeholder\n    x = array_ops.placeholder(dtype, shape=shape, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 2143, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 7400, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\sadam\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batches):\n",
    "        path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "        word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "        y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "        \n",
    "#         path_dict = tf.convert_to_tensor(path_dict)\n",
    "#         word_dict = tf.convert_to_tensor(word_dict)\n",
    "#         pos_dict = tf.convert_to_tensor(pos_dict)\n",
    "#         dep_dict = tf.convert_to_tensor(dep_dict)\n",
    "#         y_dict = tf.convert_to_tensor(y_dict)\n",
    "        \n",
    "        feed_dict = {\n",
    "            path_length:path_dict,\n",
    "            word_ids:word_dict,\n",
    "            pos_ids:pos_dict,\n",
    "            dep_ids:dep_dict,\n",
    "            y:y_dict}\n",
    "        with tf.Session(graph=grap) as sess:\n",
    "            _, loss, step = sess.run([optimizer, total_loss, global_step], feed_dict)\n",
    "        if step%10==0:\n",
    "            print(\"Step:\", step, \"loss:\",loss)\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, model_dir + '/model')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training accuracy\n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"training accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(data_dir + '/test_paths', 'rb')\n",
    "word_p1, word_p2, dep_p1, dep_p2, pos_p1, pos_p2 = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "relations = []\n",
    "for line in open(data_dir + '/test_relations.txt'):\n",
    "    relations.append(line.strip().split()[0])\n",
    "\n",
    "length = len(word_p1)\n",
    "num_batches = int(length/batch_size)\n",
    "\n",
    "for i in range(length):\n",
    "    for j, word in enumerate(word_p1[i]):\n",
    "        word = word.lower()\n",
    "        word_p1[i][j] = word if word in word2id else unknown_token \n",
    "    for k, word in enumerate(word_p2[i]):\n",
    "        word = word.lower()\n",
    "        word_p2[i][k] = word if word in word2id else unknown_token \n",
    "    for l, d in enumerate(dep_p1[i]):\n",
    "        dep_p1[i][l] = d if d in dep2id else 'OTH'\n",
    "    for m, d in enumerate(dep_p2[i]):\n",
    "        dep_p2[i][m] = d if d in dep2id else 'OTH'\n",
    "\n",
    "word_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "word_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "pos_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p1_ids = np.ones([length, max_len_path],dtype=int)\n",
    "dep_p2_ids = np.ones([length, max_len_path],dtype=int)\n",
    "rel_ids = np.array([rel2id[rel] for rel in relations])\n",
    "path1_len = np.array([len(w) for w in word_p1], dtype=int)\n",
    "path2_len = np.array([len(w) for w in word_p2])\n",
    "\n",
    "for i in range(length):\n",
    "    for j, w in enumerate(word_p1[i]):\n",
    "        word_p1_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(word_p2[i]):\n",
    "        word_p2_ids[i][j] = word2id[w]\n",
    "    for j, w in enumerate(pos_p1[i]):\n",
    "        pos_p1_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(pos_p2[i]):\n",
    "        pos_p2_ids[i][j] = pos_tag(w)\n",
    "    for j, w in enumerate(dep_p1[i]):\n",
    "        dep_p1_ids[i][j] = dep2id[w]\n",
    "    for j, w in enumerate(dep_p2[i]):\n",
    "        dep_p2_ids[i][j] = dep2id[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "all_predictions = []\n",
    "for j in range(num_batches):\n",
    "    path_dict = [path1_len[j*batch_size:(j+1)*batch_size], path2_len[j*batch_size:(j+1)*batch_size]]\n",
    "    word_dict = [word_p1_ids[j*batch_size:(j+1)*batch_size], word_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    pos_dict = [pos_p1_ids[j*batch_size:(j+1)*batch_size], pos_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    dep_dict = [dep_p1_ids[j*batch_size:(j+1)*batch_size], dep_p2_ids[j*batch_size:(j+1)*batch_size]]\n",
    "    y_dict = rel_ids[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "    feed_dict = {\n",
    "        path_length:path_dict,\n",
    "        word_ids:word_dict,\n",
    "        pos_ids:pos_dict,\n",
    "        dep_ids:dep_dict,\n",
    "        y:y_dict}\n",
    "    batch_predictions = sess.run(predictions, feed_dict)\n",
    "    all_predictions.append(batch_predictions)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(num_batches):\n",
    "    for pred in all_predictions[i]:\n",
    "        y_pred.append(pred)\n",
    "\n",
    "count = 0\n",
    "for i in range(batch_size*num_batches):\n",
    "    count += y_pred[i]==rel_ids[i]\n",
    "accuracy = count/(batch_size*num_batches) * 100\n",
    "\n",
    "print(\"test accuracy\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
