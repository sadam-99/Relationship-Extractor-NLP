{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Functions for NLP Project Part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PH51o9i5MwT",
        "outputId": "c1384d04-5575-4369-9e6a-17c06994745c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "TokenNLP = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from spacy import displacy\n",
        "from IPython.core.display import display, HTML\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg3AeVYI5WKB"
      },
      "source": [
        "path = '/content/train_2.txt'\n",
        "file1 = open('/content/train_2.txt', 'r')\n",
        "Lines = file1.readlines()\n",
        "\n",
        "def clean_str(text):\n",
        "    text = text.lower()\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"that's\", \"that is \", text)\n",
        "    text = re.sub(r\"there's\", \"there is \", text)\n",
        "    text = re.sub(r\"it's\", \"it is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"can not \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    return text.strip()\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRjTTWs5vwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4ad964-91fc-40f6-aa32-abd462daa95d"
      },
      "source": [
        "\n",
        "def preprocess():\n",
        "  sentenceList = []\n",
        "  relationList = []\n",
        "  lines = [line.strip() for line in open(path)]\n",
        "  max_sentence_length = 0\n",
        "  for idx in range(0, len(lines), 4):\n",
        "    id = lines[idx].split(\"\\t\")[0]\n",
        "    relation = lines[idx + 1]\n",
        "    sentence = lines[idx].split(\"\\t\")[1][1:-1]\n",
        "    sentence = sentence.replace('<e1>', ' _e11_ ')\n",
        "    sentence = sentence.replace('</e1>', ' _e12_ ')\n",
        "    sentence = sentence.replace('<e2>', ' _e21_ ')\n",
        "    sentence = sentence.replace('</e2>', ' _e22_ ')\n",
        "    sentence = clean_str(sentence)\n",
        "    relationList.append(relation)\n",
        "    sentenceList.append(sentence)\n",
        "  return relationList, sentenceList\n",
        "\n",
        "relationList, sentenceList = preprocess()\n",
        "print(sentenceList)\n",
        "# print(len(sentenceList))\n",
        "# for i in range(len(sentenceList)):\n",
        "#   print(i, \" \", sentenceList[i])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['e11 thom yorke e12 of e21 radiohead e22 has included the + for many of his signature distortion sounds using a variety of guitars to achieve various tonal options']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-po5xeEqHAVz",
        "outputId": "d13dd487-a32f-4ce2-fd36-55d5de42dbfb"
      },
      "source": [
        "def getRelationsOfSentences():\n",
        "  relationMap = {}\n",
        "  relationCount = 0\n",
        "  relationList, sentenceList = preprocess()\n",
        "  for sentence in sentenceList:\n",
        "    e11Index = sentence.index('e11') + 3\n",
        "    e12Index = sentence.index('e12')\n",
        "    firstEntity = sentence[e11Index : e12Index]\n",
        "\n",
        "    e21Index = sentence.index('e21') + 3\n",
        "    e22Index = sentence.index('e22') \n",
        "    secondEntity = sentence[e21Index : e22Index]\n",
        "    #print(firstEntity, \" \", secondEntity, \" \", relationList[relationCount])\n",
        "    relationMap[relationCount] = [firstEntity, secondEntity, relationList[relationCount]]\n",
        "    relationCount += 1\n",
        "  return relationMap\n",
        "  \n",
        "\n",
        "relationMap1 = getRelationsOfSentences()\n",
        "print(relationMap1)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [' thom yorke ', ' radiohead ', 'per:employee_of(e1,e2)']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH2XxyH24EU8",
        "outputId": "431be330-c8f7-4753-de1a-cdbdb547de55"
      },
      "source": [
        "\n",
        "def tokenize(sentenceList):\n",
        "  sentenceCount = 0\n",
        "  tokenMap = {}\n",
        "  for sentence in sentenceList:\n",
        "    #print(sentenceCount, \" \", sentence)\n",
        "    sentence = sentence.replace('e11', '').replace('e12','').replace('e21','').replace('e22','')\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tokenMap[sentenceCount] = tokens\n",
        "    sentenceCount += 1\n",
        "  return tokenMap\n",
        "  \n",
        "relationList, sentenceList = preprocess()\n",
        "#print(len(sentenceList))\n",
        "tokenMap = tokenize(sentenceList)\n",
        "print(tokenMap)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ['thom', 'yorke', 'of', 'radiohead', 'has', 'included', 'the', '+', 'for', 'many', 'of', 'his', 'signature', 'distortion', 'sounds', 'using', 'a', 'variety', 'of', 'guitars', 'to', 'achieve', 'various', 'tonal', 'options']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_gHD_IX5VNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6cc002a-12dc-4a48-c17a-e65264de9317"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer() \n",
        "def lemmatize(tokenMap):\n",
        "  lemmaMap = {}\n",
        "  lemmaCount = 0\n",
        "  for tokenList in tokenMap.values():\n",
        "    lemmaList = []\n",
        "    #print(\" tokenList \", tokenList , \"/n\")\n",
        "    for token in tokenList:\n",
        "      lemmaList.append(lemmatizer.lemmatize(token))\n",
        "    lemmaMap[lemmaCount] = lemmaList\n",
        "    lemmaCount += 1\n",
        "  \n",
        "  return lemmaMap\n",
        "\n",
        "relationList, sentenceList = preprocess()\n",
        "#print(len(sentenceList))\n",
        "tokenMap = tokenize(sentenceList)\n",
        "lemmaMap = lemmatize(tokenMap)\n",
        "print(lemmaMap)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ['thom', 'yorke', 'of', 'radiohead', 'ha', 'included', 'the', '+', 'for', 'many', 'of', 'his', 'signature', 'distortion', 'sound', 'using', 'a', 'variety', 'of', 'guitar', 'to', 'achieve', 'various', 'tonal', 'option']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvoYfRZvKLKQ",
        "outputId": "fc83bb28-4178-424b-cd4f-5682953afe1d"
      },
      "source": [
        "\n",
        "def posTags(tokenMap):\n",
        "  posMap = {}\n",
        "  posCount = 0\n",
        "  for tokenList in tokenMap.values():\n",
        "    posTags= nltk.pos_tag(tokenList)\n",
        "    #print(\"posTags \", posTags)\n",
        "    posMap[posCount] = posTags\n",
        "    posCount += 1\n",
        "  \n",
        "  return posMap\n",
        "\n",
        "relationList, sentenceList = preprocess()\n",
        "tokenMap = tokenize(sentenceList)\n",
        "posMap = posTags(tokenMap)\n",
        "print(posMap)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [('thom', 'JJ'), ('yorke', 'NN'), ('of', 'IN'), ('radiohead', 'NN'), ('has', 'VBZ'), ('included', 'VBN'), ('the', 'DT'), ('+', 'NNP'), ('for', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('his', 'PRP$'), ('signature', 'NN'), ('distortion', 'NN'), ('sounds', 'VBZ'), ('using', 'VBG'), ('a', 'DT'), ('variety', 'NN'), ('of', 'IN'), ('guitars', 'NNS'), ('to', 'TO'), ('achieve', 'VB'), ('various', 'JJ'), ('tonal', 'JJ'), ('options', 'NNS')]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhYAeQADLdZa",
        "outputId": "6eb4ffbe-6f5e-484c-c2ae-2bbe26878bf5"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer() \n",
        "def getHyponymsAndHypernyms(tokenMap):\n",
        "  for sentence in sentenceList:\n",
        "    sentence = sentence.replace('e11', \" \").replace('e12',\" \").replace('e21',\" \").replace('e22',\" \")\n",
        "    tokendoc = TokenNLP(sentence)\n",
        "    print(\"sentence is \", sentence)\n",
        "    for token in tokendoc:\n",
        "      print(\" token is \", token)\n",
        "      for ss in wn.synsets(token.lemma_):\n",
        "        \n",
        "        print (\"hypernyms: \"+str(ss.hypernyms())+\" hyponyms: \" + str(ss.hyponyms())+\" holonyms: \" + str(ss.member_holonyms())+\" meronyms: \"+ str(ss.part_meronyms()))\n",
        "          \n",
        "    \n",
        "    \n",
        "  \n",
        "\n",
        "relationList, sentenceList = preprocess()\n",
        "#print(len(sentenceList))\n",
        "#tokenMap = tokenize(sentenceList)\n",
        "getHyponymsAndHypernyms(sentenceList)\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sentence is    thom yorke   of   radiohead   has included the + for many of his signature distortion sounds using a variety of guitars to achieve various tonal options\n",
            " token is    \n",
            " token is  thom\n",
            " token is  yorke\n",
            " token is    \n",
            " token is  of\n",
            " token is    \n",
            " token is  radiohead\n",
            " token is    \n",
            " token is  has\n",
            "hypernyms: [Synset('person.n.01')] hyponyms: [Synset('affluent.n.01'), Synset('billionaire.n.01'), Synset('croesus.n.02'), Synset('fat_cat.n.01'), Synset('man_of_means.n.01'), Synset('millionaire.n.01'), Synset('millionairess.n.01'), Synset('multi-billionaire.n.01'), Synset('plutocrat.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('bear.v.11'), Synset('carry.v.21'), Synset('keep.v.03'), Synset('keep.v.07'), Synset('keep.v.19'), Synset('keep.v.20'), Synset('monopolize.v.02'), Synset('stock.v.01'), Synset('sustain.v.04'), Synset('wield.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('abound.v.02'), Synset('bear.v.01'), Synset('brim.v.01'), Synset('bristle.v.03'), Synset('carry.v.02'), Synset('carry.v.18'), Synset('carry.v.22'), Synset('carry.v.35'), Synset('give_off.v.01'), Synset('imply.v.05'), Synset('possess.v.01'), Synset('read.v.02'), Synset('sport.v.01'), Synset('star.v.01'), Synset('unite.v.03'), Synset('wear.v.02'), Synset('wear.v.03'), Synset('wear.v.05')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('undergo.v.01')] hyponyms: [Synset('horripilate.v.01'), Synset('suffer.v.02'), Synset('take.v.15')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('prepossess.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('make.v.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('cannibalize.v.01'), Synset('drink.v.01'), Synset('drink.v.02'), Synset('drug.v.02'), Synset('eat.v.01'), Synset('eat.v.02'), Synset('feed.v.06'), Synset('hit.v.15'), Synset('partake.v.03'), Synset('sample.v.01'), Synset('satiate.v.01'), Synset('smoke.v.01'), Synset('sup.v.01'), Synset('swallow.v.01'), Synset('take_in.v.14'), Synset('use.v.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('interact.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('direct.v.04')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('leave.v.11')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('change.v.02')] hyponyms: [Synset('meet.v.11')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('suffer.v.06')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('bring.v.11'), Synset('compel.v.01'), Synset('decide.v.03'), Synset('encourage.v.03'), Synset('lead.v.05'), Synset('let.v.02'), Synset('persuade.v.02'), Synset('prompt.v.02'), Synset('solicit.v.04'), Synset('suborn.v.03')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('get.v.01')] hyponyms: [Synset('accept.v.05'), Synset('adopt.v.02'), Synset('honor.v.03'), Synset('welcome.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('get.v.01')] hyponyms: [Synset('accept.v.09'), Synset('fence.v.02'), Synset('graduate.v.01'), Synset('hustle.v.04'), Synset('inherit.v.03'), Synset('take_in.v.12')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('experience.v.03')] hyponyms: [Synset('break_down.v.08'), Synset('crack_up.v.01'), Synset('cramp.v.04')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('score.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('produce.v.01')] hyponyms: [Synset('calve.v.02'), Synset('cub.v.01'), Synset('drop.v.23'), Synset('farrow.v.01'), Synset('fawn.v.03'), Synset('foal.v.01'), Synset('have_a_bun_in_the_oven.v.01'), Synset('kitten.v.01'), Synset('lamb.v.01'), Synset('litter.v.03'), Synset('twin.v.04'), Synset('whelp.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('sleep_together.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  included\n",
            "hypernyms: [] hyponyms: [Synset('embrace.v.01'), Synset('hold.v.11'), Synset('incorporate.v.02'), Synset('inhere_in.v.01'), Synset('involve.v.05'), Synset('subsume.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('see.v.05')] hyponyms: [Synset('carry.v.12'), Synset('count.v.06'), Synset('subsume.v.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('add.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('permit.v.01')] hyponyms: [Synset('initiate.v.03'), Synset('involve.v.02'), Synset('readmit.v.02')] holonyms: [] meronyms: []\n",
            " token is  the\n",
            " token is  +\n",
            " token is  for\n",
            " token is  many\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  of\n",
            " token is  his\n",
            " token is  signature\n",
            "hypernyms: [Synset('name.n.01')] hyponyms: [Synset('allograph.n.02'), Synset('autograph.n.02'), Synset('countersignature.n.01'), Synset('endorsement.n.04'), Synset('sign_manual.n.01')] holonyms: [] meronyms: [Synset('paraph.n.01')]\n",
            "hypernyms: [Synset('manner.n.01')] hyponyms: [Synset('common_touch.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('tune.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('musical_notation.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('sheet.n.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  distortion\n",
            "hypernyms: [Synset('damage.n.01')] hyponyms: [Synset('warp.n.03')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('shape.n.02')] hyponyms: [Synset('knot.n.04'), Synset('tortuosity.n.01'), Synset('warp.n.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('optical_phenomenon.n.01')] hyponyms: [Synset('chromatic_aberration.n.01'), Synset('spherical_aberration.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('acoustic_phenomenon.n.01'), Synset('electrical_phenomenon.n.01')] hyponyms: [Synset('nonlinear_distortion.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('falsification.n.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('mistake.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  sounds\n",
            "hypernyms: [Synset('sound_property.n.01')] hyponyms: [Synset('noisiness.n.01'), Synset('ring.n.01'), Synset('unison.n.03'), Synset('voice.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('sensation.n.01')] hyponyms: [Synset('dub.n.01'), Synset('music.n.02'), Synset('music.n.04'), Synset('noise.n.02'), Synset('tone.n.07')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('mechanical_phenomenon.n.01')] hyponyms: [Synset('ultrasound.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('happening.n.01')] hyponyms: [Synset('beat.n.06'), Synset('beep.n.01'), Synset('bell.n.03'), Synset('bong.n.01'), Synset('buzz.n.01'), Synset('chink.n.03'), Synset('chirp.n.01'), Synset('chirrup.n.01'), Synset('chorus.n.01'), Synset('click-clack.n.01'), Synset('clip-clop.n.01'), Synset('cry.n.05'), Synset('ding.n.01'), Synset('drip.n.02'), Synset('drum.n.02'), Synset('footfall.n.01'), Synset('gargle.n.02'), Synset('gurgle.n.01'), Synset('jingle.n.01'), Synset('knock.n.01'), Synset('mutter.n.01'), Synset('noise.n.01'), Synset('paradiddle.n.01'), Synset('pat.n.01'), Synset('patter.n.02'), Synset('peal.n.01'), Synset('ping.n.02'), Synset('plunk.n.01'), Synset('pop.n.03'), Synset('purr.n.01'), Synset('quack.n.02'), Synset('quaver.n.01'), Synset('ring.n.06'), Synset('rub-a-dub.n.01'), Synset('sigh.n.02'), Synset('skirl.n.01'), Synset('song.n.02'), Synset('strum.n.01'), Synset('susurration.n.01'), Synset('swish.n.01'), Synset('tapping.n.01'), Synset('throbbing.n.02'), Synset('thrum.n.01'), Synset('thump.n.01'), Synset('thunk.n.01'), Synset('tick.n.01'), Synset('ting.n.01'), Synset('toot.n.01'), Synset('tootle.n.01'), Synset('trample.n.01'), Synset('twang.n.01'), Synset('vibrato.n.01'), Synset('voice.n.03'), Synset('vroom.n.01'), Synset('whack.n.01'), Synset('whir.n.01'), Synset('whistle.n.01'), Synset('whiz.n.02'), Synset('zing.n.01'), Synset('zizz.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('auditory_communication.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('language_unit.n.01')] hyponyms: [Synset('consonant.n.01'), Synset('orinasal_phone.n.01'), Synset('phoneme.n.01'), Synset('semivowel.n.01'), Synset('sonant.n.01'), Synset('vowel.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('channel.n.04')] hyponyms: [Synset('narrow.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('body_of_water.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('look.v.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('cause_to_be_perceived.v.01')] hyponyms: [Synset('bang.v.02'), Synset('bleep.v.01'), Synset('blow.v.05'), Synset('blow.v.10'), Synset('boom.v.01'), Synset('boom.v.04'), Synset('buzz.v.01'), Synset('chatter.v.01'), Synset('chime.v.01'), Synset('chug.v.01'), Synset('clang.v.01'), Synset('clangor.v.01'), Synset('clank.v.01'), Synset('click.v.02'), Synset('clop.v.01'), Synset('crack.v.02'), Synset('crash.v.07'), Synset('drone.v.01'), Synset('drum.v.01'), Synset('glug.v.01'), Synset('guggle.v.02'), Synset('gurgle.v.02'), Synset('honk.v.01'), Synset('hum.v.03'), Synset('lap.v.03'), Synset('make_noise.v.01'), Synset('patter.v.02'), Synset('ping.v.03'), Synset('pink.v.02'), Synset('pop.v.03'), Synset('rattle.v.01'), Synset('resonate.v.01'), Synset('resound.v.01'), Synset('ring.v.01'), Synset('ripple.v.02'), Synset('roll.v.05'), Synset('rumble.v.01'), Synset('rustle.v.01'), Synset('skirl.v.01'), Synset('snap.v.06'), Synset('snarl.v.02'), Synset('splash.v.05'), Synset('splat.v.01'), Synset('squelch.v.02'), Synset('tap.v.07'), Synset('thud.v.01'), Synset('tick.v.02'), Synset('ting.v.02'), Synset('tinkle.v.01'), Synset('trump.v.01'), Synset('twang.v.02'), Synset('tweet.v.01'), Synset('whish.v.01'), Synset('whistle.v.01'), Synset('whistle.v.05'), Synset('whizz.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('cackel.v.01'), Synset('dissonate.v.01'), Synset('pierce.v.03'), Synset('play.v.13'), Synset('speak.v.05')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('announce.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('pronounce.v.01')] hyponyms: [Synset('chirk.v.01'), Synset('quaver.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('blow.v.11'), Synset('clink.v.01'), Synset('gong.v.01'), Synset('play.v.07'), Synset('pop.v.05'), Synset('prepare.v.07'), Synset('ring.v.03'), Synset('strum.v.01'), Synset('ting.v.01'), Synset('twang.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('quantify.v.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  using\n",
            "hypernyms: [Synset('activity.n.01')] hyponyms: [Synset('application.n.01'), Synset('exploitation.n.01'), Synset('misuse.n.01'), Synset('play.n.06'), Synset('practice.n.04'), Synset('recycling.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('utility.n.02')] hyponyms: [Synset('raison_d'etre.n.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('utility.n.02')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('demand.n.02')] hyponyms: [Synset('conspicuous_consumption.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('custom.n.01')] hyponyms: [Synset('cleanliness.n.01'), Synset('ritual.n.03'), Synset('second_nature.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('influence.n.02')] hyponyms: [Synset('mind_game.n.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('legal_right.n.01')] hyponyms: [Synset('fair_use.n.01'), Synset('fruition.n.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('address.v.05'), Synset('avail.v.01'), Synset('cannibalize.v.02'), Synset('enjoy.v.02'), Synset('exert.v.01'), Synset('exploit.v.01'), Synset('exploit.v.02'), Synset('fall_back.v.05'), Synset('give.v.18'), Synset('implement.v.01'), Synset('misapply.v.01'), Synset('overuse.v.01'), Synset('play.v.32'), Synset('play.v.33'), Synset('ply.v.06'), Synset('pull_out_all_the_stops.v.01'), Synset('put.v.04'), Synset('recycle.v.02'), Synset('share.v.02'), Synset('strain.v.03'), Synset('take.v.09'), Synset('waste.v.02'), Synset('work.v.12')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('consume.v.02')] hyponyms: [Synset('board.v.03'), Synset('drink.v.05')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('abuse.v.04'), Synset('pervert.v.03'), Synset('spare.v.04'), Synset('take.v.02'), Synset('waste.v.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('exploit.v.01')] hyponyms: [Synset('trespass.v.02')] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [Synset('follow.v.19')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('act.v.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  a\n",
            "hypernyms: [Synset('metric_linear_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('picometer.n.01')]\n",
            "hypernyms: [Synset('fat-soluble_vitamin.n.01')] hyponyms: [Synset('vitamin_a1.n.01'), Synset('vitamin_a2.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('nucleotide.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('purine.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('current_unit.n.01')] hyponyms: [] holonyms: [] meronyms: [Synset('milliampere.n.01')]\n",
            "hypernyms: [Synset('letter.n.02')] hyponyms: [] holonyms: [Synset('roman_alphabet.n.01')] meronyms: []\n",
            "hypernyms: [Synset('blood_group.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  variety\n",
            "hypernyms: [Synset('collection.n.01')] hyponyms: [Synset('alphabet_soup.n.01'), Synset('grab_bag.n.01'), Synset('odds_and_ends.n.01'), Synset('range.n.06'), Synset('sampler.n.03'), Synset('selection.n.02'), Synset('witches'_brew.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('heterogeneity.n.01')] hyponyms: [Synset('biodiversity.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('taxonomic_group.n.01')] hyponyms: [Synset('breed.n.01'), Synset('cultivar.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('show.n.03')] hyponyms: [Synset('minstrel_show.n.02'), Synset('revue.n.01'), Synset('vaudeville.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('category.n.02')] hyponyms: [Synset('antitype.n.02'), Synset('art_form.n.01'), Synset('brand.n.02'), Synset('color.n.07'), Synset('description.n.03'), Synset('flavor.n.03'), Synset('genre.n.01'), Synset('genus.n.01'), Synset('like.n.01'), Synset('like.n.02'), Synset('manner.n.03'), Synset('model.n.02'), Synset('species.n.02'), Synset('stripe.n.04'), Synset('style.n.03'), Synset('type.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('difference.n.01')] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  of\n",
            " token is  guitars\n",
            "hypernyms: [Synset('stringed_instrument.n.01')] hyponyms: [Synset('acoustic_guitar.n.01'), Synset('bass_guitar.n.01'), Synset('cittern.n.01'), Synset('electric_guitar.n.01'), Synset('hawaiian_guitar.n.01'), Synset('uke.n.01')] holonyms: [] meronyms: [Synset('fingerboard.n.03')]\n",
            " token is  to\n",
            " token is  achieve\n",
            "hypernyms: [Synset('succeed.v.01')] hyponyms: [Synset('average.v.02'), Synset('begin.v.09'), Synset('come_to.v.03'), Synset('compass.v.01'), Synset('culminate.v.03'), Synset('reach.v.07'), Synset('score.v.06'), Synset('wangle.v.01')] holonyms: [] meronyms: []\n",
            " token is  various\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  tonal\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            "hypernyms: [] hyponyms: [] holonyms: [] meronyms: []\n",
            " token is  options\n",
            "hypernyms: [Synset('derivative_instrument.n.01')] hyponyms: [Synset('call_option.n.02'), Synset('covered_option.n.01'), Synset('incentive_option.n.01'), Synset('lock-up_option.n.01'), Synset('naked_option.n.01'), Synset('put_option.n.02'), Synset('stock_option.n.01'), Synset('straddle.n.04')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('decision_making.n.01')] hyponyms: [Synset('default_option.n.01'), Synset('hobson's_choice.n.01'), Synset('impossibility.n.02'), Synset('obverse.n.01'), Synset('possibility.n.04'), Synset('preference.n.03'), Synset('soft_option.n.01')] holonyms: [] meronyms: []\n",
            "hypernyms: [Synset('action.n.01')] hyponyms: [Synset('casting.n.04'), Synset('coloration.n.03'), Synset('decision.n.01'), Synset('election.n.02'), Synset('sampling.n.01'), Synset('volition.n.02'), Synset('vote.n.01')] holonyms: [] meronyms: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0UrAMlFN3L8",
        "outputId": "25d63586-5cc7-4d02-ec5f-9c1ce22083ec"
      },
      "source": [
        "def NERTags(tokenMap):\n",
        "  NERMap = {}\n",
        "  NERCount = 0\n",
        "  for firstEntity, secondEntity, relationList in getRelationsOfSentences().values():\n",
        "    docFirst = nlp(firstEntity)\n",
        "    docSecond = nlp(secondEntity)\n",
        "    NERList = []\n",
        "    #print(\"posTags \", posTags)\n",
        "    for ent in docFirst.ents:\n",
        "      NERList.append((ent.text, ent.label_))\n",
        "    for ent in docSecond.ents:\n",
        "      NERList.append((ent.text, ent.label_))\n",
        "    NERMap[NERCount] = NERList\n",
        "    NERCount += 1\n",
        "  \n",
        "  return NERMap\n",
        "\n",
        "\n",
        "NERMap = NERTags(getRelationsOfSentences())\n",
        "print(NERMap)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: []}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJf-3LW0UWPB",
        "outputId": "01759161-899c-4546-89b7-d08ec675c0cb"
      },
      "source": [
        "\n",
        "def dependencyTree(sentenceList):\n",
        "  \n",
        "  dependecyMap = {}\n",
        "  dependecyCount = 0\n",
        "  for sentence in sentenceList:\n",
        "    sentence = sentence.replace('e11', \" \").replace('e12',\" \").replace('e21',\" \").replace('e22',\" \")\n",
        "    depedenlist = []\n",
        "    tokendoc = TokenNLP(sentence)\n",
        "    for token in tokendoc:\n",
        "      depedenlist.append((token,token.dep_))\n",
        "    dependecyMap[dependecyCount] = depedenlist\n",
        "    dependecyCount += 1\n",
        "    html = displacy.render(tokendoc, style=\"dep\")\n",
        "    # Uncomment below 2 lines\n",
        "    # print(\" Sentence is \", sentence)\n",
        "    # display(HTML(html))\n",
        "  print(dependecyMap)\n",
        "\n",
        "relationList, sentenceList = preprocess()\n",
        "dependencyTree(sentenceList)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: [(  , ''), (thom, 'compound'), (yorke, 'nsubj'), (  , ''), (of, 'prep'), (  , ''), (radiohead, 'pobj'), (  , ''), (has, 'aux'), (included, 'ROOT'), (the, 'det'), (+, 'dobj'), (for, 'mark'), (many, 'nsubj'), (of, 'prep'), (his, 'poss'), (signature, 'compound'), (distortion, 'pobj'), (sounds, 'conj'), (using, 'xcomp'), (a, 'det'), (variety, 'dobj'), (of, 'prep'), (guitars, 'pobj'), (to, 'aux'), (achieve, 'xcomp'), (various, 'amod'), (tonal, 'amod'), (options, 'dobj')]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7gyHYKfay0P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}